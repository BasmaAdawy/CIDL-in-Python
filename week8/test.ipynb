{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimization Algorithms: Evolutionary Strategies, Genetic Algorithms, and Simulated Annealing**\n",
        "\n",
        "This notebook provides a Python implementation of foundational metaheuristic optimization algorithms: **Evolutionary Strategies (ES)**, **Genetic Algorithms (GA)**, and **Simulated Annealing (SA)**. These algorithms are designed to tackle complex optimization problems, particularly those involving non-linear, non-differentiable, or high-dimensional objective functions where traditional gradient-based methods may struggle.\n",
        "\n",
        "The implementations focus on minimizing a 1D real-valued function within a bounded interval, using both **binary encoding** (for ES and GA) and **real encoding** (for SA). Key components such as chromosome representation, fitness evaluation, mutation, crossover, and selection operators are defined and demonstrated.\n",
        "\n",
        "This resource is intended for academic and educational purposes, providing a clear, commented, and self-contained reference for understanding and experimenting with these powerful optimization paradigms."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D # Although 3D plotting is not central to 1D optimization, included for consistency with prior notebooks.\n",
        "\n",
        "# Set a random seed for reproducibility of results\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "setup_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Binary Encoding Utilities**\n",
        "\n",
        "For evolutionary algorithms like Genetic Algorithms and certain Evolutionary Strategies, representing solutions as **binary strings** (chromosomes) is a common practice. This section defines utility functions for working with this binary encoding: converting binary chromosomes to real-valued numbers, initializing random populations, and evaluating the fitness of these encoded solutions.\n",
        "\n",
        "## **1.1 Logical to Real Conversion**\n",
        "The conversion from a binary vector to a real-valued number within a defined interval $[a, b]$ is crucial. A binary string of length $L$ can represent $2^L$ unique integer values. These integer values are then mapped linearly to the continuous interval. Each binary string corresponds to an unsigned integer value. For a binary vector $\\mathbf{v} = [v_0, v_1, \\ldots, v_{L-1}]$ where $v_i \\in \\{0, 1\\}$ and $v_0$ is the most significant bit:\n",
        "$$ \\text{uint_value} = \\sum_{i=0}^{L-1} v_i \\cdot 2^{(L-1-i)} $$\n",
        "This integer value is then scaled to the interval $[0, 1)$:\n",
        "$$ \\text{real_value}_{0,1} = \\frac{\\text{uint_value}}{2^L} $$\n",
        "Finally, this value is mapped to the desired interval $[a, b)$ with a small offset to center the discrete values:\n",
        "$$ \\text{real_value} = a + \\text{real_value}_{0,1} \\cdot (b-a) + \\frac{0.5 \\cdot (b-a)}{2^L} $$"
      ],
      "metadata": {
        "id": "binary_encoding_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logical2real(logical_vector, a, b):\n",
        "    \"\"\"\n",
        "    Converts a binary-encoded (logical) vector to a real-valued number\n",
        "    within a specified interval [a, b). The mapping is designed to distribute\n",
        "    values uniformly across the interval.\n",
        "\n",
        "    Parameters:\n",
        "    - logical_vector: A 1D NumPy array of booleans (binary chromosome).\n",
        "    - a: Lower bound of the real-valued interval.\n",
        "    - b: Upper bound of the real-valued interval.\n",
        "\n",
        "    Returns:\n",
        "    - real_value: The converted real-valued number.\n",
        "    \"\"\"\n",
        "    len_vector = len(logical_vector)\n",
        "    \n",
        "    # Calculate the unsigned integer value represented by the binary vector.\n",
        "    # The most significant bit (MSB) is at index 0 (leftmost).\n",
        "    # For Python (0-indexed): sum(2**(len_vector - 1 - i)) for i where logical_vector[i] is True.\n",
        "    powers_of_2 = 2**(np.arange(len_vector - 1, -1, -1)) # Generates weights [2^(L-1), 2^(L-2), ..., 2^0]\n",
        "    uint_value = np.sum(logical_vector * powers_of_2) # Sum of weighted bits where the bit is True\n",
        "    \n",
        "    # Map the integer value to the interval [0, 1)\n",
        "    # This represents the fractional part of the real number\n",
        "    real_value_0_1 = uint_value / (2**len_vector)\n",
        "    \n",
        "    # Scale and shift to the desired interval (a, b)\n",
        "    # The '+ 0.5*(b-a)/(2**len_vector)' term is to center the discrete values\n",
        "    # within their respective sub-intervals, effectively mapping to (a, b) midpoint.\n",
        "    real_value = a + real_value_0_1 * (b - a) + 0.5 * (b - a) / (2**len_vector)\n",
        "    \n",
        "    return real_value\n"
      ],
      "metadata": {
        "id": "logical2real_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BE_initpop(ngenes, popsize=1):\n",
        "    \"\"\"\n",
        "    Initializes a population of binary-encoded individuals with random gene values.\n",
        "\n",
        "    Parameters:\n",
        "    - ngenes: The number of genes (length of the binary chromosome) for each individual.\n",
        "    - popsize: The number of individuals in the population (default is 1).\n",
        "\n",
        "    Returns:\n",
        "    - initpop: A NumPy array of boolean values representing the initial population.\n",
        "               Each row is an individual's chromosome (shape: popsize x ngenes).\n",
        "    \"\"\"\n",
        "    # Generates a random boolean array where each element is True or False with 50% probability\n",
        "    initpop = np.random.randint(0, 2, size=(popsize, ngenes), dtype=bool)\n",
        "    return initpop\n"
      ],
      "metadata": {
        "id": "BE_initpop_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BE_evaluatefitness(f, P, a, b):\n",
        "    \"\"\"\n",
        "    Evaluates the fitness (objective function value) for each individual in the population.\n",
        "    For minimization problems, a lower fitness value indicates a better solution.\n",
        "\n",
        "    Parameters:\n",
        "    - f: Function handle (callable) of the objective function to minimize.\n",
        "    - P: Population matrix, where each row is a binary-encoded individual (popsize x ngenes).\n",
        "    - a: Lower bound of the real-valued search space.\n",
        "    - b: Upper bound of the real-valued search space.\n",
        "\n",
        "    Returns:\n",
        "    - fit: A 1D NumPy array containing the fitness value for each individual (shape: popsize,).\n",
        "    \"\"\"\n",
        "    fit = np.zeros(P.shape[0]) # Initialize fitness array for each individual\n",
        "    for i in range(P.shape[0]):\n",
        "        # Convert binary-encoded individual to real value and then evaluate its fitness\n",
        "        fit[i] = f(logical2real(P[i, :], a, b))\n",
        "    return fit\n"
      ],
      "metadata": {
        "id": "BE_evaluatefitness_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Genetic Algorithm (GA) Components**\n",
        "\n",
        "Genetic Algorithms simulate natural selection and genetics to find optimal solutions. This section details the core operators that drive the GA's search process: **mutation**, **crossover**, and **selection**.\n",
        "\n",
        "## **2.1 Uniform Mutation (Binary Encoding)**\n",
        "Uniform mutation alters individual genes (bits) of a chromosome with a specified probability. For each gene in a chromosome $\\mathbf{x} = [x_0, x_1, \\ldots, x_{L-1}]$:\n",
        "$$ x_i' = \\begin{cases} 1 - x_i & \\text{with probability } p_{mut} \\\\ x_i & \\text{with probability } 1 - p_{mut} \\end{cases} $$\n",
        "where $p_{mut}$ is the mutation probability for each gene."
      ],
      "metadata": {
        "id": "ga_components_mutation_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BE_MUT_unif_all(logical_x, mutprob):\n",
        "    \"\"\"\n",
        "    Performs uniform mutation on a binary-encoded chromosome. Each gene (bit)\n",
        "    in the chromosome has an independent probability 'mutprob' of flipping its value.\n",
        "\n",
        "    Parameters:\n",
        "    - logical_x: The binary-encoded chromosome (1D NumPy array of booleans).\n",
        "    - mutprob: The probability of mutation for each gene (between 0 and 1).\n",
        "\n",
        "    Returns:\n",
        "    - mutated_x: The mutated chromosome as a new 1D NumPy array of booleans.\n",
        "    \"\"\"\n",
        "    len_x = len(logical_x)\n",
        "    # Generate a random probability for each gene (element) in the chromosome\n",
        "    prob = np.random.rand(len_x)\n",
        "    # Identify the indices of genes that should be flipped (where random prob < mutprob)\n",
        "    ind_of_genes_to_mutate = np.where(prob < mutprob)[0]\n",
        "    \n",
        "    mutated_x = np.copy(logical_x) # Create a copy to ensure the original chromosome is not modified directly\n",
        "    # Flip the values (True to False, False to True) of the selected genes\n",
        "    mutated_x[ind_of_genes_to_mutate] = np.logical_not(logical_x[ind_of_genes_to_mutate])\n",
        "    return mutated_x\n"
      ],
      "metadata": {
        "id": "BE_MUT_unif_all_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Single-Point Crossover (Binary Encoding)**\n",
        "Single-point crossover creates two offspring from two parents by selecting a random crossover point and swapping the segments of the chromosomes after that point.\n",
        "Given two parents, $P_1 = [p_{1,0}, \\ldots, p_{1,L-1}]$ and $P_2 = [p_{2,0}, \\ldots, p_{2,L-1}]$, and a random split point $s$ ($0 < s < L-1$):\n",
        "$$ O_1 = [p_{1,0}, \\ldots, p_{1,s-1}, p_{2,s}, \\ldots, p_{2,L-1}] $$\n",
        "$$ O_2 = [p_{2,0}, \\ldots, p_{2,s-1}, p_{1,s}, \\ldots, p_{1,L-1}] $$\n",
        "where $O_1$ and $O_2$ are the two offspring."
      ],
      "metadata": {
        "id": "ga_components_crossover_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BE_XOVER_singlepoint(parent1, parent2):\n",
        "    \"\"\"\n",
        "    Performs single-point crossover between two binary-encoded parents.\n",
        "    A random crossover point is chosen, and the genetic material is swapped\n",
        "    between the parents after this point to generate two new offspring.\n",
        "\n",
        "    Parameters:\n",
        "    - parent1: The first parent's chromosome (1D NumPy array of booleans).\n",
        "    - parent2: The second parent's chromosome (1D NumPy array of booleans).\n",
        "\n",
        "    Returns:\n",
        "    - offsprings: A 2xL NumPy array of booleans, where the first row is Child 1\n",
        "                  and the second row is Child 2.\n",
        "    - splitpoint: The index of the crossover point. Useful for analysis/debugging.\n",
        "    \"\"\"\n",
        "    len_chromosome = len(parent1)\n",
        "    \n",
        "    # Randomly select a split point (index). The point can be from 1 to len_chromosome - 2\n",
        "    # to ensure there's at least one gene on each side of the split.\n",
        "    splitpoint = np.random.randint(1, len_chromosome - 1)\n",
        "    \n",
        "    # Initialize an array to hold the two offspring chromosomes\n",
        "    offsprings = np.full((2, len_chromosome), False, dtype=bool)\n",
        "\n",
        "    # Create the first offspring\n",
        "    offsprings[0, :splitpoint] = parent1[:splitpoint] # First part from parent1\n",
        "    offsprings[0, splitpoint:] = parent2[splitpoint:] # Second part from parent2\n",
        "\n",
        "    # Create the second offspring\n",
        "    offsprings[1, :splitpoint] = parent2[:splitpoint] # First part from parent2\n",
        "    offsprings[1, splitpoint:] = parent1[splitpoint:] # Second part from parent1\n",
        "\n",
        "    return offsprings, splitpoint\n"
      ],
      "metadata": {
        "id": "BE_XOVER_singlepoint_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Binary Tournament Selection**\n",
        "Binary tournament selection is a widely used method for choosing individuals to participate in reproduction. For each slot in the mating pool, two individuals are randomly selected from the current population. The individual with the **better fitness** (lower value for minimization problems) is chosen and added to the mating pool. This process is repeated until the mating pool is filled to the desired `popsize`."
      ],
      "metadata": {
        "id": "ga_components_selection_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binarytournement(P, fit_P, popsize):\n",
        "    \"\"\"\n",
        "    Performs binary tournament selection to create a mating pool.\n",
        "    In each tournament, two individuals are randomly chosen from the population.\n",
        "    The individual with the superior fitness (for minimization, the lower value)\n",
        "    is selected to be part of the mating pool.\n",
        "\n",
        "    Parameters:\n",
        "    - P: Current population matrix (shape: current_popsize x ngenes).\n",
        "    - fit_P: Fitness values for each individual in P (shape: current_popsize,).\n",
        "    - popsize: The desired size of the mating pool.\n",
        "\n",
        "    Returns:\n",
        "    - MP: The mating pool, a new population of selected individuals (shape: popsize x ngenes).\n",
        "    \"\"\"\n",
        "    ngenes = P.shape[1] # Number of genes (chromosome length)\n",
        "    MP = np.full((popsize, ngenes), False, dtype=bool) # Initialize the mating pool\n",
        "    \n",
        "    current_pop_size = P.shape[0]\n",
        "\n",
        "    for i in range(popsize):\n",
        "        # Randomly select two unique indices for competitors from the current population\n",
        "        competitors_indices = np.random.choice(current_pop_size, 2, replace=False)\n",
        "        \n",
        "        # Compare their fitness values and select the better one\n",
        "        if fit_P[competitors_indices[0]] < fit_P[competitors_indices[1]]:\n",
        "            MP[i, :] = P[competitors_indices[0], :]\n",
        "        else:\n",
        "            MP[i, :] = P[competitors_indices[1], :]\n",
        "    return MP\n"
      ],
      "metadata": {
        "id": "binarytournement_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Optimization Algorithms Implementations**\n",
        "\n",
        "This section presents the implementations of three distinct optimization algorithms: Evolutionary Strategy, Genetic Algorithm, and Simulated Annealing. Each algorithm is designed to find the minimum of a given 1D objective function. A common test function, `difficult2minimafcn1D`, is introduced first."
      ],
      "metadata": {
        "id": "optimization_algorithms_main_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Difficult Test Function**\n",
        "The `difficult2minimafcn1D` is a 1D function designed to challenge optimization algorithms due to its multiple local minima, where the global minimum might be non-trivial to locate. It is defined piecewise:\n",
        "$$ f(x) = \\begin{cases} -e^{-(x/100)^2} & x \\le 100 \\\\ -e^{-1} + (x-100)(x-102) & x > 100 \\end{cases} $$\n",
        "This function exhibits a smoother, Gaussian-like behavior for $x \\le 100$ and a quadratic, parabolic-like behavior for $x > 100$. The transition at $x=100$ creates a potential challenge for search methods."
      ],
      "metadata": {
        "id": "difficult_function_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def difficult2minimafcn1D(x):\n",
        "    \"\"\"\n",
        "    A 1D test function with two minima, where the global minimum\n",
        "    is rather difficult to find. Its mathematical description is provided\n",
        "    in the markdown cell above.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input value(s) (can be a scalar or a NumPy array).\n",
        "\n",
        "    Returns:\n",
        "    - y: Output value(s) of the function.\n",
        "    \"\"\"\n",
        "    # Ensure x is a NumPy array for vectorized operations, handling scalar inputs gracefully\n",
        "    x_arr = np.asarray(x)\n",
        "    y = np.zeros_like(x_arr, dtype=float)\n",
        "\n",
        "    # Apply the first part of the piecewise function for x <= 100\n",
        "    ind_le100 = x_arr <= 100\n",
        "    y[ind_le100] = -np.exp(-(x_arr[ind_le100] / 100)**2)\n",
        "\n",
        "    # Apply the second part of the piecewise function for x > 100\n",
        "    ind_gt100 = x_arr > 100\n",
        "    y[ind_gt100] = -np.exp(-1) + (x_arr[ind_gt100] - 100) * (x_arr[ind_gt100] - 102)\n",
        "    \n",
        "    # Return scalar if input was scalar, otherwise return array\n",
        "    return y.item() if x_arr.ndim == 0 else y\n"
      ],
      "metadata": {
        "id": "difficult_function_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 (1+1) Evolution Strategy with Binary Encoding (ES-BE)**\n",
        "\n",
        "The **(1+1) Evolution Strategy** is a simple yet powerful evolutionary algorithm. It operates with a single parent solution that generates one child solution through mutation. The child then competes with the parent, and the better of the two (for minimization, the one with lower fitness) survives to become the parent of the next generation. This strategy is 'elitist' because the best solution found so far is always preserved or improved upon.\n",
        "\n",
        "The core loop involves:\n",
        "1.  **Mutation:** A child $\\mathbf{x}'$ is generated from the current parent $\\mathbf{x}$ using a mutation operator, often by flipping bits with a certain probability.\n",
        "2.  **Selection:** If $f(\\mathbf{x}') < f(\\mathbf{x})$, then $\\mathbf{x}'$ replaces $\\mathbf{x}$. Otherwise, $\\mathbf{x}$ is retained.\n",
        "\n",
        "### Algorithm Summary:\n",
        "1.  Initialize a random parent $\\mathbf{x}$.\n",
        "2.  For $t = 1, \\ldots, \\text{niter}$:\n",
        "    a.  Create a child $\\mathbf{x}'$ by mutating $\\mathbf{x}$.\n",
        "    b.  Evaluate fitness $f(\\mathbf{x})$ and $f(\\mathbf{x}')$.\n",
        "    c.  If $f(\\mathbf{x}') < f(\\mathbf{x})$, set $\\mathbf{x} \\leftarrow \\mathbf{x}'$.\n",
        "3.  Return the final $\\mathbf{x}$ as the optimum."
      ],
      "metadata": {
        "id": "es_be_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ES_BE_1D(f, a, b, niter, ngenes, mutprob, dispAndPlot=True):\n",
        "    \"\"\"\n",
        "    Implements a (1+1) Evolution Strategy for 1D continuous function minimization,\n",
        "    using a binary encoding for the solution. This is an elitist strategy where the\n",
        "    better of the parent and child survives.\n",
        "\n",
        "    Parameters:\n",
        "    - f: Function handle (callable) of the objective function to minimize.\n",
        "    - a: Lower bound of the real-valued search space.\n",
        "    - b: Upper bound of the real-valued search space.\n",
        "    - niter: Number of iterations (generations) to run the evolution.\n",
        "    - ngenes: Length of the binary chromosome for encoding the 1D variable.\n",
        "    - mutprob: Probability of mutation for each gene.\n",
        "    - dispAndPlot: Boolean, if True, displays real-time progress and plots.\n",
        "\n",
        "    Returns:\n",
        "    - x_opt: The best real-valued solution found.\n",
        "    - f_opt: The fitness (objective function value) of the best solution.\n",
        "    \"\"\"\n",
        "    # Initialize the current parent individual randomly, encoded as a binary vector\n",
        "    current_x_binary = BE_initpop(ngenes, 1) # Returns (1, ngenes) array\n",
        "    \n",
        "    # Evaluate the fitness of the initial parent\n",
        "    current_fitness = BE_evaluatefitness(f, current_x_binary, a, b)[0] # Extract scalar fitness from the 1-element array\n",
        "    \n",
        "    # Initialize the best solution found so far with the initial parent\n",
        "    best_x_overall = logical2real(current_x_binary[0], a, b) # Convert 1D binary chromosome to real value\n",
        "    best_y_overall = current_fitness\n",
        "\n",
        "    if dispAndPlot:\n",
        "        # Prepare plot for function visualization and search progress\n",
        "        domain = np.linspace(a, b, 1000) # Domain for plotting the objective function\n",
        "        colmap = plt.cm.autumn(np.linspace(0, 1, niter)) # Colormap for visualizing search points over time\n",
        "        \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        \n",
        "        # Subplot 1: Objective function landscape\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(domain, f(domain), '-b', linewidth=2)\n",
        "        plt.title('Objective Function Landscape')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('f(x)')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Subplot 2: Search progress of the Evolutionary Strategy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        # Plot the initial solution as a distinctive marker\n",
        "        plt.plot(logical2real(current_x_binary[0], a, b), current_fitness, 'sk', markersize=8, label='Initial Solution')\n",
        "        plt.plot(domain, f(domain), '-b', alpha=0.5, label='Function Landscape') # Plot function background\n",
        "        plt.title('(1+1) Evolutionary Strategy Search Progress')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('f(x)')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.ion() # Turn on interactive mode for live plotting updates\n",
        "        plt.show()\n",
        "\n",
        "    for i in range(niter): # Loop through specified number of iterations\n",
        "        # Generate a new child solution by mutating the current parent's chromosome\n",
        "        # BE_MUT_unif_all expects a 1D array (chromosome), so use current_x_binary[0]\n",
        "        new_x_child_binary_1D = BE_MUT_unif_all(current_x_binary[0], mutprob)\n",
        "        # Reshape the child chromosome back to (1, ngenes) for fitness evaluation\n",
        "        new_x_child_binary_2D = new_x_child_binary_1D.reshape(1, -1) \n",
        "        \n",
        "        # Evaluate the fitness of the new child solution\n",
        "        new_fitness_child = BE_evaluatefitness(f, new_x_child_binary_2D, a, b)[0] # Extract scalar fitness\n",
        "\n",
        "        # (1+1) Selection: If the child's fitness is better than the parent's,\n",
        "        # the child replaces the parent for the next iteration.\n",
        "        if new_fitness_child < current_fitness:\n",
        "            current_x_binary = new_x_child_binary_2D # Child becomes new parent\n",
        "            current_fitness = new_fitness_child\n",
        "            # If this new parent is also the best solution found so far, update overall bests\n",
        "            if current_fitness < best_y_overall:\n",
        "                best_y_overall = current_fitness\n",
        "                best_x_overall = logical2real(current_x_binary[0], a, b)\n",
        "\n",
        "        if dispAndPlot:\n",
        "            # Plot the new child solution's position in the search space\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(logical2real(new_x_child_binary_1D, a, b), new_fitness_child, 'o', \n",
        "                     color=colmap[i], markersize=5, alpha=0.6) # Use colormap for iteration progression\n",
        "            plt.draw() # Update the plot\n",
        "            plt.pause(0.01) # Small pause for animation effect\n",
        "            \n",
        "            # Print current iteration's progress and overall best found\n",
        "            print(f'Iteration {i+1}/{niter}: Global Best x = {best_x_overall:.4f}, f(x) = {best_y_overall:.4f} | ' \n",
        "                  f'Evaluated Child x = {logical2real(new_x_child_binary_1D, a, b):.4f}, f(x) = {new_fitness_child:.4f}')\n",
        "    \n",
        "    if dispAndPlot:\n",
        "        plt.ioff() # Turn off interactive mode at the end\n",
        "        plt.show() # Display the final plot\n",
        "\n",
        "    # The final best solution is derived from the last 'current_x' and 'current_fitness'\n",
        "    x_opt = logical2real(current_x_binary[0], a, b)\n",
        "    f_opt = current_fitness\n",
        "\n",
        "    return x_opt, f_opt\n"
      ],
      "metadata": {
        "id": "ES_BE_1D_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3 Canonical Genetic Algorithm with Binary Encoding (GA-BE)**\n",
        "\n",
        "A **Canonical Genetic Algorithm (GA)** evolves a population of solutions over generations. It uses selection, crossover, and mutation operators to produce new, potentially better, generations. This implementation includes elitism, where the best individuals from the current generation are guaranteed to be carried over to the next.\n",
        "\n",
        "### Algorithm Summary:\n",
        "1.  **Initialization:** Create an initial population $P$ of `popsize` random chromosomes.\n",
        "2.  **Evaluation:** Compute the fitness of each individual in $P$.\n",
        "3.  For $t = 1, \\ldots, \\text{generations}$:\n",
        "    a.  **Selection:** Create a Mating Pool (MP) by selecting individuals from $P$ based on their fitness (e.g., using binary tournament selection).\n",
        "    b.  **Crossover:** Pair individuals from MP and apply a crossover operator (e.g., single-point crossover) to create offspring population $Q$.\n",
        "    c.  **Mutation:** Apply a mutation operator (e.g., uniform mutation) to each gene in $Q$ with a small probability.\n",
        "    d.  **Evaluation:** Compute the fitness of each individual in $Q$.\n",
        "    e.  **Replacement (Elitism):** Form the next generation $P'$ by combining $P$ and $Q$ and selecting the `popsize` best individuals.\n",
        "4.  Return the best individual found in the final population."
      ],
      "metadata": {
        "id": "ga_be_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GA_BE_1D(f, a, b, generations, chrsize, popsize, pmutallgenes, displayAndPlot=False):\n",
        "    \"\"\"\n",
        "    Implements a simple Canonical Genetic Algorithm (GA) with Binary Encoding\n",
        "    for the minimization of a real function in one dimension, within a specified interval (a, b).\n",
        "    This version is elitist, propagating the best individuals to the next generation.\n",
        "\n",
        "    Parameters:\n",
        "    - f: Function handle (callable) of the objective function to optimize.\n",
        "    - a: Lower bound of the real-valued search space.\n",
        "    - b: Upper bound of the real-valued search space.\n",
        "    - generations: The number of generations (iterations) for the GA.\n",
        "    - chrsize: The length of the binary chromosome for each individual.\n",
        "               A larger size increases resolution but also the search space complexity.\n",
        "    - popsize: The size of the population. Must be an even number for standard crossover.\n",
        "    - pmutallgenes: The probability of mutation for each gene (uniform mutation).\n",
        "    - displayAndPlot: Boolean, if True, displays plots of fitness progression.\n",
        "\n",
        "    Returns:\n",
        "    - x_opt: The best real-valued solution found.\n",
        "    - f_opt: The fitness (objective function value) of the best solution.\n",
        "    - best_fit_history: A NumPy array containing the best fitness found at each generation.\n",
        "    \"\"\"\n",
        "    if popsize % 2 != 0:\n",
        "        raise ValueError('Population size (popsize) must be an even number for pairwise crossover.')\n",
        "\n",
        "    # 1. Generate the initial random population\n",
        "    P = BE_initpop(chrsize, popsize) # P will be (popsize, chrsize)\n",
        "\n",
        "    # 2. Compute the fitness of each individual in the initial population\n",
        "    fit_P = BE_evaluatefitness(f, P, a, b) # fit_P will be (popsize,)\n",
        "\n",
        "    # Initialize the array to store the best fitness found at each generation\n",
        "    best_fit_history = np.zeros(generations) # Stores the minimum fitness per generation\n",
        "    best_fit_history[0] = np.min(fit_P)\n",
        "\n",
        "    print(f\"Starting Genetic Algorithm for {generations} generations with population size {popsize}...\")\n",
        "\n",
        "    for t in range(1, generations): # Iterate from the second generation (index 1) up to 'generations'\n",
        "        # 3a. Selection: Create the Mating Pool (MP) using binary tournament selection\n",
        "        # The MP has the same size as the current population P\n",
        "        MP = binarytournement(P, fit_P, popsize)\n",
        "\n",
        "        # 3b. Reshuffle the Mating Pool to create random pairings for crossover\n",
        "        shuffled_indices = np.random.permutation(popsize)\n",
        "        MP = MP[shuffled_indices, :]\n",
        "\n",
        "        # 3c. Crossover: Apply the single-point crossover operator to each pair in the MP\n",
        "        # to obtain the offspring population Q\n",
        "        Q = np.full((popsize, chrsize), False, dtype=bool) # Initialize offspring population\n",
        "        for i in range(0, popsize, 2): # Iterate over parents in pairs\n",
        "            parent1 = MP[i, :]\n",
        "            parent2 = MP[i+1, :]\n",
        "            # BE_XOVER_singlepoint returns a (2, chrsize) array of two offspring\n",
        "            offsprings, _ = BE_XOVER_singlepoint(parent1, parent2)\n",
        "            Q[i, :] = offsprings[0, :]   # Assign first offspring\n",
        "            Q[i+1, :] = offsprings[1, :] # Assign second offspring\n",
        "        \n",
        "        # 3d. Mutation: Apply the uniform mutation operator to each individual in Q\n",
        "        for i in range(popsize):       \n",
        "           Q[i, :] = BE_MUT_unif_all(Q[i, :], pmutallgenes) # Mutate each offspring chromosome\n",
        "        \n",
        "        # 3e. Evaluation: Compute the fitness of the offspring population Q\n",
        "        fit_Q = BE_evaluatefitness(f, Q, a, b)\n",
        "        \n",
        "        # 3f. Replacement (Elitism): Combine current population (P) and offspring (Q) into R\n",
        "        # Then, select the best 'popsize' individuals from R to form the new population\n",
        "        R = np.vstack((P, Q)) # Vertically stack parent and offspring populations\n",
        "        fit_R = np.hstack((fit_P, fit_Q)) # Horizontally stack their fitness values\n",
        "        \n",
        "        # Sort individuals by fitness in ascending order (for minimization)\n",
        "        sorted_indices = np.argsort(fit_R)\n",
        "        \n",
        "        # Select the top 'popsize' individuals to become the next generation P\n",
        "        P = R[sorted_indices[:popsize], :]\n",
        "        fit_P = fit_R[sorted_indices[:popsize]]\n",
        "        \n",
        "        # 3g. Update best fitness history for the current generation\n",
        "        best_fit_history[t] = np.min(fit_P)\n",
        "        \n",
        "        print(f\"Generation {t+1}/{generations}, Best Fitness: {best_fit_history[t]:.6f}\")\n",
        "\n",
        "    # After all generations, find the overall best solution in the final population\n",
        "    f_opt = np.min(fit_P)\n",
        "    idx_opt = np.argmin(fit_P)\n",
        "    x_opt = logical2real(P[idx_opt, :], a, b) # Convert the best binary chromosome to real value\n",
        "\n",
        "    if displayAndPlot:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        \n",
        "        # Subplot 1: Best fitness found during the first few generations\n",
        "        plt.subplot(1, 2, 1)\n",
        "        first_gen_display = min(30, generations) # Show only first 30 generations or total generations if less\n",
        "        plt.plot(np.arange(1, first_gen_display + 1), best_fit_history[:first_gen_display], '-b', linewidth=2)\n",
        "        plt.xlim([1, first_gen_display])\n",
        "        plt.title(f'Best Fitness During First {first_gen_display} Generations')\n",
        "        plt.xlabel('Generation')\n",
        "        plt.ylabel('Fitness (f(x))')\n",
        "        plt.grid(True)\n",
        "        # Set y-axis limits to be consistent with the full generations plot for comparison\n",
        "        # This needs the other subplot's limits, so might be better to set after the other plot is defined\n",
        "\n",
        "        # Subplot 2: Best fitness found across all generations\n",
        "        ax_full_generations = plt.subplot(1, 2, 2)\n",
        "        plt.plot(np.arange(1, generations + 1), best_fit_history, '-b', linewidth=2)\n",
        "        plt.xlim([1, generations])\n",
        "        plt.title('Best Fitness Found at Each Generation (Full Run)')\n",
        "        plt.xlabel('Generation')\n",
        "        plt.ylabel('Fitness (f(x))')\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # Now set the y-limit for the first subplot to match the second one\n",
        "        plt.subplot(1, 2, 1).set_ylim(ax_full_generations.get_ylim())\n",
        "\n",
        "        plt.tight_layout() # Adjust layout to prevent overlapping titles/labels\n",
        "        plt.show()\n",
        "\n",
        "    return x_opt, f_opt, best_fit_history\n"
      ],
      "metadata": {
        "id": "GA_BE_1D_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4 Simulated Annealing with Real Encoding (SA-RE)**\n",
        "\n",
        "**Simulated Annealing (SA)** is a probabilistic metaheuristic inspired by the annealing process in metallurgy, where a material is heated and then slowly cooled to increase its crystal size and perfect its structure, thereby reducing defects. In optimization, this translates to exploring the search space at high \"temperatures\" (early iterations, allowing greater exploration and acceptance of worse solutions) and gradually reducing the temperature (later iterations, favoring exploitation and better solutions). This strategy helps in escaping local minima.\n",
        "\n",
        "### Key Concepts:\n",
        "* **Temperature Schedule ($T_t$):** A function that controls the 'cooling' process, typically decreasing over iterations. A common schedule is exponential decay: $T_t = T_0 \\cdot e^{-\\lambda t}$, where $T_0$ is the initial temperature, $\\lambda$ is the decay rate, and $t$ is the iteration number.\n",
        "* **Perturbation:** A mechanism to generate a new candidate solution from the current one, often by adding a small random step.\n",
        "* **Acceptance Probability:** The likelihood of accepting a new solution, especially if it's worse than the current one. This is typically governed by the **Metropolis Criterion**:\n",
        "    $$ P(\\text{accept}) = \\begin{cases} 1 & \\text{if } f(\\mathbf{x}') < f(\\mathbf{x}) \\\\ e^{-\\frac{f(\\mathbf{x}') - f(\\mathbf{x})}{T_t}} & \\text{if } f(\\mathbf{x}') \\ge f(\\mathbf{x}) \\end{cases} $$\n",
        "    where $f(\\mathbf{x})$ is the current fitness, $f(\\mathbf{x}')$ is the candidate fitness, and $T_t$ is the current temperature.\n",
        "\n",
        "### Algorithm Summary:\n",
        "1.  Initialize a random current solution $\\mathbf{x}$ and the best solution found $\\mathbf{x}_{best}$.\n",
        "2.  Define a temperature schedule $T_t$ for $t = 1, \\ldots, \\text{niter}$.\n",
        "3.  For $t = 1, \\ldots, \\text{niter}$:\n",
        "    a.  Generate a new candidate solution $\\mathbf{x}'$ by perturbing $\\mathbf{x}$.\n",
        "    b.  Evaluate fitness $f(\\mathbf{x}')$.\n",
        "    c.  Decide whether to accept $\\mathbf{x}'$ as the new current solution based on the Metropolis Criterion.\n",
        "    d.  If $f(\\mathbf{x}') < f(\\mathbf{x}_{best})$, update $\\mathbf{x}_{best} \\leftarrow \\mathbf{x}'$.\n",
        "4.  Return $\\mathbf{x}_{best}$ as the estimated optimum."
      ],
      "metadata": {
        "id": "sa_re_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb(current_x, a, b):\n",
        "    \"\"\"\n",
        "    Generates a new candidate solution by perturbing the current solution.\n",
        "    The perturbation is a random step, scaled by a fraction of the search interval width,\n",
        "    and clipped to ensure the new solution stays within the defined bounds [a, b].\n",
        "\n",
        "    Parameters:\n",
        "    - current_x: The current real-valued solution.\n",
        "    - a: Lower bound of the search space.\n",
        "    - b: Upper bound of the search space.\n",
        "\n",
        "    Returns:\n",
        "    - new_x: The perturbed candidate solution.\n",
        "    \"\"\"\n",
        "    # Add a random step to the current solution. The step size is proportional to the interval width (b-a).\n",
        "    # The `(np.random.rand() - 0.5)` term ensures the step is centered around zero.\n",
        "    new_x = current_x + 0.2 * (b - a) * (np.random.rand() - 0.5)\n",
        "    \n",
        "    # Clip (constrain) the new solution to ensure it remains within the specified bounds [a, b]\n",
        "    new_x = np.clip(new_x, a, b)\n",
        "    \n",
        "    return new_x\n",
        "\n",
        "def SA_RE_1D(f, a, b, niter, decay_rate, dispAndPlot=False):\n",
        "    \"\"\"\n",
        "    Implements the Simulated Annealing (SA) optimization algorithm\n",
        "    for 1D continuous function minimization, using a real-valued encoding.\n",
        "\n",
        "    Parameters:\n",
        "    - f: Function handle (callable) of the objective function to minimize.\n",
        "    - a: Lower bound of the real-valued search space.\n",
        "    - b: Upper bound of the real-valued search space.\n",
        "    - niter: Number of iterations (cooling steps).\n",
        "    - decay_rate: Lambda (λ), the exponential decay rate for the temperature schedule.\n",
        "    - dispAndPlot: Boolean, if True, displays real-time progress and plots.\n",
        "\n",
        "    Returns:\n",
        "    - best_x: The best real-valued solution found over all iterations.\n",
        "    - best_y: The fitness (objective function value) of the best solution found.\n",
        "    \"\"\"\n",
        "    # Define the temperature schedule: exponential decay based on iteration number.\n",
        "    # T_t = T_0 * exp(-decay_rate * t). Here, T_0 is implicitly 1 as it scales the energy difference.\n",
        "    temperature_schedule = np.exp(-decay_rate * np.arange(1, niter + 1))\n",
        "    \n",
        "    # Initialize the current solution randomly within the search bounds [a, b]\n",
        "    current_x = np.random.uniform(a, b)\n",
        "    current_y = f(current_x) # Evaluate fitness of the initial current solution\n",
        "\n",
        "    # Initialize the best solution found so far with the initial current solution\n",
        "    best_x = current_x\n",
        "    best_y = current_y\n",
        "\n",
        "    print(f\"Starting Simulated Annealing for {niter} iterations...\")\n",
        "\n",
        "    if dispAndPlot:\n",
        "        # Prepare plotting setup: 3 subplots for temperature, function, and search progress\n",
        "        domain = np.linspace(a, b, 1000) # Domain for plotting the objective function\n",
        "        colmap = plt.cm.autumn(np.linspace(0, 1, niter)) # Colormap for visualizing search points\n",
        "        \n",
        "        plt.figure(figsize=(18, 6))\n",
        "        \n",
        "        # Subplot 1: Temperature schedule visualization\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(np.arange(1, niter + 1), temperature_schedule, '-r', linewidth=2)\n",
        "        plt.title('Temperature vs. Iteration (Cooling Schedule)')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Temperature')\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # Subplot 2: Objective function landscape\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(domain, f(domain), '-b', linewidth=2)\n",
        "        plt.title('Objective Function Landscape')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('f(x)')\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # Subplot 3: Simulated Annealing search progress\n",
        "        plt.subplot(1, 3, 3)\n",
        "        # Plot the initial solution as a distinctive marker\n",
        "        plt.plot(current_x, current_y, 'sk', markersize=8, label='Initial Solution')\n",
        "        plt.plot(domain, f(domain), '-b', alpha=0.5, label='Function Landscape') # Plot function background\n",
        "        plt.title('Simulated Annealing Search Progress')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('f(x)')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.ion() # Turn on interactive mode for live plotting\n",
        "        plt.show()\n",
        "\n",
        "    for i in range(niter): # Loop through specified number of iterations\n",
        "        # Generate a new candidate solution by perturbing the current one\n",
        "        new_x = perturb(current_x, a, b)\n",
        "        new_y = f(new_x) # Evaluate fitness of the new candidate solution\n",
        "\n",
        "        # Decision to accept the new solution using the Metropolis Criterion\n",
        "        # If new_y is better (lower) than current_y, always accept.\n",
        "        # If new_y is worse, accept with a probability that decreases with temperature.\n",
        "        if new_y < current_y: # If the new solution is better\n",
        "            current_x = new_x\n",
        "            current_y = new_y\n",
        "        else: # If the new solution is worse or equal\n",
        "            # Calculate acceptance probability based on fitness difference and current temperature\n",
        "            delta_E = new_y - current_y # Energy difference (positive for worse solutions)\n",
        "            current_temp = temperature_schedule[i] # Get temperature for current iteration\n",
        "            acceptance_probability = np.exp(-delta_E / current_temp)\n",
        "            \n",
        "            # Accept the worse solution stochastically\n",
        "            if np.random.rand() < acceptance_probability:\n",
        "                current_x = new_x\n",
        "                current_y = new_y\n",
        "        \n",
        "        # Update the overall best solution found so far (based on any candidate, not just accepted ones)\n",
        "        if new_y < best_y: \n",
        "            best_x = new_x\n",
        "            best_y = new_y\n",
        "\n",
        "        if dispAndPlot:\n",
        "            # Plot the new candidate solution in the search space\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.plot(new_x, new_y, 'o', color=colmap[i], markersize=5, alpha=0.6) # Use colormap for progression\n",
        "            plt.draw() # Update the plot\n",
        "            plt.pause(0.01) # Small pause for animation effect\n",
        "\n",
        "            # Print current iteration's progress and overall best found\n",
        "            print(f'Iteration {i+1}/{niter}: Global Best x = {best_x:.4f}, f(x) = {best_y:.4f} | ' \n",
        "                  f'Current x = {current_x:.4f}, f(x) = {current_y:.4f}')\n",
        "    \n",
        "    if dispAndPlot:\n",
        "        plt.ioff() # Turn off interactive mode at the end\n",
        "        plt.show() # Display the final plot\n",
        "\n",
        "    return best_x, best_y\n"
      ],
      "metadata": {
        "id": "SA_RE_1D_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Main Execution and Examples**\n",
        "\n",
        "This section demonstrates how to use the implemented optimization algorithms to find the minimum of various 1D test functions. Examples for the Rastrigin function, a sinusoidal function, and the `difficult2minimafcn1D` are provided for each algorithm."
      ],
      "metadata": {
        "id": "main_execution_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1 Example: (1+1) Evolution Strategy (ES-BE)**\n",
        "Demonstrating ES-BE on common 1D test functions."
      ],
      "metadata": {
        "id": "es_example_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- (1+1) Evolution Strategy (ES-BE) Examples ---\")\n",
        "\n",
        "# Example 1 - Rastrigin Function\n",
        "print(\"\\nRunning ES-BE on Rastrigin function...\")\n",
        "f_rastrigin = lambda x: 10 + x**2 - 10 * np.cos(2 * np.pi * x)\n",
        "a_rastrigin = -5\n",
        "b_rastrigin = 5\n",
        "niter_es = 1000\n",
        "ngenes_es = 20 # Increased genes for better resolution\n",
        "mutprob_es = 0.05 # Reduced mutation probability for stability\n",
        "disp_es = True\n",
        "\n",
        "print(f'Known optimum for Rastrigin: x = 0.0 (y = 0.0)')\n",
        "[best_x_es_rastrigin, best_y_es_rastrigin] = ES_BE_1D(f_rastrigin, a_rastrigin, b_rastrigin, niter_es, ngenes_es, mutprob_es, disp_es)\n",
        "print(f'ES-BE found for Rastrigin: x_opt = {best_x_es_rastrigin:.4f}, f_opt = {best_y_es_rastrigin:.4f}')\n",
        "\n",
        "\n",
        "# Example 2 - Sinusoidal shaped function\n",
        "print(\"\\nRunning ES-BE on Sinusoidal function...\")\n",
        "f_sinusoidal = lambda x: np.sin(x) + np.sin((10.0 / 3.0) * x)\n",
        "a_sinusoidal = -2.7\n",
        "b_sinusoidal = 7.5\n",
        "niter_es_sin = 1500 # More iterations\n",
        "ngenes_es_sin = 20\n",
        "mutprob_es_sin = 0.05\n",
        "disp_es_sin = True\n",
        "\n",
        "print(f'Known optimum for Sinusoidal: x = 5.145735 (y = -1.8996)')\n",
        "[best_x_es_sinusoidal, best_y_es_sinusoidal] = ES_BE_1D(f_sinusoidal, a_sinusoidal, b_sinusoidal, niter_es_sin, ngenes_es_sin, mutprob_es_sin, disp_es_sin)\n",
        "print(f'ES-BE found for Sinusoidal: x_opt = {best_x_es_sinusoidal:.4f}, f_opt = {best_y_es_sinusoidal:.4f}')\n"
      ],
      "metadata": {
        "id": "es_example_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2 Example: Canonical Genetic Algorithm (GA-BE)**\n",
        "Demonstrating GA-BE on common 1D test functions."
      ],
      "metadata": {
        "id": "ga_example_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Canonical Genetic Algorithm (GA-BE) Examples ---\")\n",
        "\n",
        "# Example 1 - Rastrigin Function\n",
        "print(\"\\nRunning GA-BE on Rastrigin function...\")\n",
        "f_rastrigin = lambda x: 10 + x**2 - 10 * np.cos(2 * np.pi * x)\n",
        "a_rastrigin = -5\n",
        "b_rastrigin = 5\n",
        "generations_ga = 200\n",
        "chrsize_ga = 20 # Chromosome size\n",
        "popsize_ga = 50 # Population size (must be even)\n",
        "pmutallgenes_ga = 0.05 # Per-gene mutation probability\n",
        "disp_ga = True\n",
        "\n",
        "print(f'Known optimum for Rastrigin: x = 0.0 (y = 0.0)')\n",
        "[best_x_ga_rastrigin, best_y_ga_rastrigin, _] = GA_BE_1D(f_rastrigin, a_rastrigin, b_rastrigin, generations_ga, chrsize_ga, popsize_ga, pmutallgenes_ga, disp_ga)\n",
        "print(f'GA-BE found for Rastrigin: x_opt = {best_x_ga_rastrigin:.4f}, f_opt = {best_y_ga_rastrigin:.4f}')\n",
        "\n",
        "# Example 2 - Sinusoidal shaped function\n",
        "print(\"\\nRunning GA-BE on Sinusoidal function...\")\n",
        "f_sinusoidal = lambda x: np.sin(x) + np.sin((10.0 / 3.0) * x)\n",
        "a_sinusoidal = -2.7\n",
        "b_sinusoidal = 7.5\n",
        "generations_ga_sin = 300\n",
        "chrsize_ga_sin = 20\n",
        "popsize_ga_sin = 50\n",
        "pmutallgenes_ga_sin = 0.05\n",
        "disp_ga_sin = True\n",
        "\n",
        "print(f'Known optimum for Sinusoidal: x = 5.145735 (y = -1.8996)')\n",
        "[best_x_ga_sinusoidal, best_y_ga_sinusoidal, _] = GA_BE_1D(f_sinusoidal, a_sinusoidal, b_sinusoidal, generations_ga_sin, chrsize_ga_sin, popsize_ga_sin, pmutallgenes_ga_sin, disp_ga_sin)\n",
        "print(f'GA-BE found for Sinusoidal: x_opt = {best_x_ga_sinusoidal:.4f}, f_opt = {best_y_ga_sinusoidal:.4f}')\n"
      ],
      "metadata": {
        "id": "ga_example_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3 Example: Simulated Annealing (SA-RE)**\n",
        "Demonstrating SA-RE on common 1D test functions."
      ],
      "metadata": {
        "id": "sa_example_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Simulated Annealing (SA-RE) Examples ---\")\n",
        "\n",
        "# Example 1 - Rastrigin Function\n",
        "print(\"\\nRunning SA-RE on Rastrigin function...\")\n",
        "f_rastrigin = lambda x: 10 + x**2 - 10 * np.cos(2 * np.pi * x)\n",
        "a_rastrigin = -5\n",
        "b_rastrigin = 5\n",
        "niter_sa = 1000\n",
        "decay_rate_sa = 0.01 # Temperature decay rate\n",
        "disp_sa = True\n",
        "\n",
        "print(f'Known optimum for Rastrigin: x = 0.0 (y = 0.0)')\n",
        "[best_x_sa_rastrigin, best_y_sa_rastrigin] = SA_RE_1D(f_rastrigin, a_rastrigin, b_rastrigin, niter_sa, decay_rate_sa, disp_sa)\n",
        "print(f'SA-RE found for Rastrigin: x_opt = {best_x_sa_rastrigin:.4f}, f_opt = {best_y_sa_rastrigin:.4f}')\n",
        "\n",
        "# Example 2 - Sinusoidal shaped function\n",
        "print(\"\\nRunning SA-RE on Sinusoidal function...\")\n",
        "f_sinusoidal = lambda x: np.sin(x) + np.sin((10.0 / 3.0) * x)\n",
        "a_sinusoidal = -2.7\n",
        "b_sinusoidal = 7.5\n",
        "niter_sa_sin = 1500\n",
        "decay_rate_sa_sin = 0.01\n",
        "disp_sa_sin = True\n",
        "\n",
        "print(f'Known optimum for Sinusoidal: x = 5.145735 (y = -1.8996)')\n",
        "[best_x_sa_sinusoidal, best_y_sa_sinusoidal] = SA_RE_1D(f_sinusoidal, a_sinusoidal, b_sinusoidal, niter_sa_sin, decay_rate_sa_sin, disp_sa_sin)\n",
        "print(f'SA-RE found for Sinusoidal: x_opt = {best_x_sa_sinusoidal:.4f}, f_opt = {best_y_sa_sinusoidal:.4f}')\n",
        "\n",
        "# Example 3 - Difficult to Minima Function 1D\n",
        "print(\"\\nRunning SA-RE on Difficult to Minima Function 1D...\")\n",
        "a_difficult = -10\n",
        "b_difficult = 103\n",
        "niter_sa_difficult = 2000 # More iterations for this challenging function\n",
        "decay_rate_sa_difficult = 0.005 # Slower cooling for better exploration\n",
        "disp_sa_difficult = True\n",
        "\n",
        "print(f'Known global optimum for Difficult Function: approximately x = 101 (y = -1.9996)')\n",
        "[best_x_sa_difficult, best_y_sa_difficult] = SA_RE_1D(difficult2minimafcn1D, a_difficult, b_difficult, niter_sa_difficult, decay_rate_sa_difficult, disp_sa_difficult)\n",
        "print(f'SA-RE found for Difficult Function: x_opt = {best_x_sa_difficult:.4f}, f_opt = {best_y_sa_difficult:.4f}')\n"
      ],
      "metadata": {
        "id": "sa_example_code",
        "execution_count": null,
        "outputs": []
      }
    }
  ]
}