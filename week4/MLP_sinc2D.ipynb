{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MLP Regression on 2D Sinc Function**\n",
        "\n",
        "This notebook applies a Multi-Layer Perceptron (MLP) for regression on the 2D sinc function, covering data generation, MLP architecture, training, and evaluation."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D \n",
        "\n",
        "np.random.seed(42) # For reproducibility\n"
      ],
      "metadata": {
        "id": "setup_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Sinc2D Data Functions**\n",
        "\n",
        "Functions for generating and visualizing the 2D sinc dataset."
      ],
      "metadata": {
        "id": "sinc2d_functions_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sinc2D_gen(Npatterns):\n",
        "    \"\"\"\n",
        "    Generates a 2D sinc function dataset.\n",
        "    \"\"\"\n",
        "    side_length = int(np.sqrt(Npatterns))\n",
        "    x1 = np.linspace(-5, 5, side_length)\n",
        "    x2 = np.linspace(-5, 5, side_length)\n",
        "    XX1, XX2 = np.meshgrid(x1, x2)\n",
        "    \n",
        "    def safe_sinc_val(val):\n",
        "        return np.where(val == 0, 1.0, np.sin(val) / val)\n",
        "\n",
        "    YY = 10 * safe_sinc_val(XX1) * safe_sinc_val(XX2)\n",
        "\n",
        "    X = np.hstack((XX1.reshape(-1, 1), XX2.reshape(-1, 1)))\n",
        "    y = YY.reshape(-1, 1)\n",
        "    \n",
        "    return X, y, XX1, XX2, YY\n",
        "\n",
        "def sinc2D_display(XX1, XX2, YY, title_suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Displays a 3D surface plot of the 2D sinc function.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(XX1, XX2, YY, cmap='viridis', edgecolor='none')\n",
        "    ax.set_title(f'Sinc2D Synthetic Dataset {title_suffix}. Domain: [-5,5]x[-5,5]')\n",
        "    ax.set_xlabel('X1')\n",
        "    ax.set_ylabel('X2')\n",
        "    ax.set_zlabel('Y')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sinc2d_functions_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. MLP Core Functions**\n",
        "\n",
        "Essential functions for MLP: activation, cost, weight initialization, forward pass, and backpropagation.\n",
        "\n",
        "## **2.1 Activation and Cost Functions**\n",
        "* **Sigmoid Activation:** $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "* **Sigmoid Derivative:** $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
        "* **Mean Squared Error (MSE) Cost:** $J = \\frac{1}{2B} \\sum_{i=1}^{B} (y_i - \\hat{y}_i)^2$\n",
        "\n",
        "## **2.2 Forward Pass**\n",
        "Input $P_{input}$ (BatchSize $\\times$ Features) to output $\\hat{Y}$ (Output $\\times$ BatchSize):\n",
        "* Hidden Layer Pre-activation: $Z_1 = W_1 A_0$\n",
        "* Output Layer Activation (Prediction): $\\hat{Y} = W_2 A_1$\n",
        "\n",
        "## **2.3 Backpropagation**\n",
        "Error signals and gradients for weight updates:\n",
        "* Output Error: $\\delta_2 = (\\hat{Y} - Y_{true})$\n",
        "* Gradient for $W_2$: $\\nabla_{W_2} J = \\delta_2 (A_1^{ext})^T$\n",
        "* Hidden Error: $\\delta_1 = (W_2^T \\delta_2)_{\\text{excluding bias}} \\odot \\sigma'(Z_1)$\n",
        "* Gradient for $W_1$: $\\nabla_{W_1} J = \\delta_1 A_0^T$"
      ],
      "metadata": {
        "id": "mlp_core_functions_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def MLP_sigmoid_derivative(rZ):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    sigma_of_rZ = MLP_sigmoid(rZ)\n",
        "    return sigma_of_rZ * (1.0 - sigma_of_rZ)\n",
        "\n",
        "def MLP_MSE_cost(y_true, y_pred, model, W1, W2):\n",
        "    \"\"\"Mean Squared Error cost function.\"\"\"\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape(1, -1)\n",
        "    cost = np.sum((y_true - y_pred)**2) / (2 * y_true.shape[1])\n",
        "    return cost\n",
        "\n",
        "def MLP_initialize_weights(model):\n",
        "    \"\"\"Initializes MLP weights uniformly in [-1, 1].\"\"\"\n",
        "    n_hidden = model['n_hidden']\n",
        "    n_features = model['n_features']\n",
        "    n_output = model['n_output']\n",
        "    W1 = 2 * np.random.rand(n_hidden, n_features + 1) - 1\n",
        "    W2 = 2 * np.random.rand(n_output, n_hidden + 1) - 1\n",
        "    return W1, W2\n",
        "\n",
        "def MLP_MSELIN_forward(P_input, W1, W2):\n",
        "    \"\"\"Performs forward pass through MLP.\"\"\"\n",
        "    batch_size = P_input.shape[0]\n",
        "    A0 = np.vstack((np.ones(batch_size), P_input.T))\n",
        "    rZ1 = W1 @ A0\n",
        "    rA1 = MLP_sigmoid(rZ1)\n",
        "    A1 = np.vstack((np.ones(batch_size), rA1))\n",
        "    rA2 = W2 @ A1\n",
        "    return rA2, A1, A0, rZ1\n",
        "\n",
        "def MLP_MSELIN_backprop(rA2, A1, A0, rZ1, Y_true, W1, W2):\n",
        "    \"\"\"Computes gradients via backpropagation.\"\"\"\n",
        "    dL_dZ2 = rA2 - Y_true\n",
        "    dL_dW2 = dL_dZ2 @ A1.T\n",
        "    dL_dA1 = W2.T @ dL_dZ2\n",
        "    sigma_prime_of_rZ1 = MLP_sigmoid_derivative(rZ1)\n",
        "    dL_drZ1 = dL_dA1[1:, :] * sigma_prime_of_rZ1\n",
        "    dL_dW1 = dL_drZ1 @ A0.T\n",
        "    return dL_dW1, dL_dW2\n",
        "\n",
        "def MLP_MSELIN_predict(P_input, W1, W2):\n",
        "    \"\"\"Predicts outputs using trained MLP.\"\"\"\n",
        "    rA2, _, _, _ = MLP_MSELIN_forward(P_input, W1, W2)\n",
        "    return rA2\n"
      ],
      "metadata": {
        "id": "mlp_core_functions_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. MLP Training Loop**\n",
        "\n",
        "The main function to train the MLP with mini-batch Gradient Descent."
      ],
      "metadata": {
        "id": "mlp_training_loop_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_MSELIN_train(P_train, y_train, model):\n",
        "    \"\"\"Trains the MLP model.\"\"\"\n",
        "    W1, W2 = MLP_initialize_weights(model)\n",
        "    eta = model['eta']\n",
        "    epochs = model['epochs']\n",
        "    minibatches = model['minibatches']\n",
        "    model['cost_history'] = []\n",
        "    num_observations = P_train.shape[0]\n",
        "    \n",
        "    for e in range(1, epochs + 1):\n",
        "        current_eta = eta # Fixed learning rate\n",
        "        shuffled_indices = np.random.permutation(num_observations)\n",
        "        P_shuffled = P_train[shuffled_indices, :]\n",
        "        y_shuffled = y_train[shuffled_indices, :]\n",
        "        mini_batch_indices = np.array_split(np.arange(num_observations), minibatches)\n",
        "                          \n",
        "        for m_idx, idx in enumerate(mini_batch_indices):\n",
        "            P_mini_batch = P_shuffled[idx, :]\n",
        "            y_mini_batch = y_shuffled[idx, :].T \n",
        "            rA2, A1, A0, rZ1 = MLP_MSELIN_forward(P_mini_batch, W1, W2)\n",
        "            cost = MLP_MSE_cost(y_mini_batch, rA2, model, W1, W2)\n",
        "            model['cost_history'].append(cost)\n",
        "            print(f'Epoch {e}/{epochs}, Minibatch {m_idx+1}/{minibatches}, Loss (MSE) {cost:.6f}')\n",
        "            delta_W1_unscaled, delta_W2_unscaled = MLP_MSELIN_backprop(rA2, A1, A0, rZ1, y_mini_batch, W1, W2)\n",
        "            delta_W1 = current_eta * delta_W1_unscaled\n",
        "            delta_W2 = current_eta * delta_W2_unscaled\n",
        "            W1 = W1 - delta_W1\n",
        "            W2 = W2 - delta_W2\n",
        "            \n",
        "    model['W1'] = W1\n",
        "    model['W2'] = W2\n",
        "    return model, W1, W2\n"
      ],
      "metadata": {
        "id": "mlp_training_loop_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Main Script: Training and Evaluation**\n",
        "\n",
        "Sets up parameters, generates data, trains the MLP, and evaluates its performance."
      ],
      "metadata": {
        "id": "main_script_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close('all') # Close all figures\n",
        "\n",
        "# Parameters\n",
        "Ninput = 2         \n",
        "Ntrain = 150**2    \n",
        "Ntest  = 2500      \n",
        "\n",
        "# Data Generation\n",
        "Xtrain, ytrain, XX1_train, XX2_train, YY_train = sinc2D_gen(Ntrain)\n",
        "sinc2D_display(XX1_train, XX2_train, YY_train, title_suffix=\"(Training Data)\")\n",
        "Xtest, ytest, _, _, _ = sinc2D_gen(Ntest)\n",
        "\n",
        "# Shuffle training data\n",
        "shuffled_ind = np.random.permutation(Ntrain)\n",
        "Xtrain = Xtrain[shuffled_ind, :]\n",
        "ytrain = ytrain[shuffled_ind]\n",
        "\n",
        "# Model Parameters\n",
        "model = {\n",
        "    'n_output': 1,        \n",
        "    'n_features': Ninput, \n",
        "    'n_hidden': 300,      \n",
        "    'epochs': 500,        \n",
        "    'eta': 1e-6,          \n",
        "    'minibatches': 30,    \n",
        "}\n",
        "\n",
        "# Train MLP\n",
        "print(\"\\nStarting MLP training...\")\n",
        "model_trained, W1_trained, W2_trained = MLP_MSELIN_train(Xtrain, ytrain, model)\n",
        "print(\"MLP training complete.\")\n",
        "\n",
        "# Predictions\n",
        "ytrain_pred = MLP_MSELIN_predict(Xtrain, W1_trained, W2_trained).T\n",
        "ytest_pred  = MLP_MSELIN_predict(Xtest,  W1_trained, W2_trained).T\n",
        "\n",
        "# Compute MSE\n",
        "acc_train = np.sum((ytrain - ytrain_pred)**2) / (2 * len(ytrain))\n",
        "print(f'\\nTraining MSE: {acc_train:.6f}')\n",
        "acc_test = np.sum((ytest - ytest_pred)**2) / (2 * len(ytest))\n",
        "print(f'Test MSE: {acc_test:.6f}')\n",
        "\n",
        "# Plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ytrain, ytrain_pred, '.b', alpha=0.5)\n",
        "plt.title('Scatter Plot (Training Set)')\n",
        "plt.xlabel('True Y')\n",
        "plt.ylabel('Predicted Y')\n",
        "plt.grid(True)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ytest, ytest_pred, '.r', alpha=0.5)\n",
        "plt.title('Scatter Plot (Test Set)')\n",
        "plt.xlabel('True Y')\n",
        "plt.ylabel('Predicted Y')\n",
        "plt.grid(True)\n",
        "plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(model_trained['cost_history'], color='purple', linewidth=2)\n",
        "plt.title('MLP Training Loss')\n",
        "plt.xlabel('Mini-batch Iteration')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualize Learned Surface\n",
        "if Ninput == 2:\n",
        "    x1_vis = np.linspace(-5, 5, 100)\n",
        "    x2_vis = np.linspace(-5, 5, 100)\n",
        "    XX1_vis, XX2_vis = np.meshgrid(x1_vis, x2_vis)\n",
        "    X_vis = np.hstack((XX1_vis.reshape(-1, 1), XX2_vis.reshape(-1, 1)))\n",
        "    YY_pred_surface = MLP_MSELIN_predict(X_vis, W1_trained, W2_trained).T.reshape(XX1_vis.shape)\n",
        "    \n",
        "    fig = plt.figure(figsize=(12, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(XX1_vis, XX2_vis, YY_pred_surface, cmap='viridis', edgecolor='none', alpha=0.8)\n",
        "    ax.set_title('MLP Learned Surface for Sinc2D Function')\n",
        "    ax.set_xlabel('X1')\n",
        "    ax.set_ylabel('X2')\n",
        "    ax.set_zlabel('Predicted Y')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "main_script_code"
      }
    }
  ]
}
