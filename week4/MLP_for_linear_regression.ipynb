{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Layer Perceptron (MLP) for Regression**\n",
        "\n",
        "This notebook provides a comprehensive implementation of a Multi-Layer Perceptron (MLP) neural network for regression tasks. It covers weight initialization, forward propagation, backpropagation for gradient computation, and the training process using mini-batch Gradient Descent. The Mean Squared Error (MSE) is used as the loss function, and the sigmoid function is employed as the activation for the hidden layer."
      ],
      "metadata": {
        "id": "mlp_intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D # Included for consistency, though not directly used in 1D MLP visualization\n",
        "\n",
        "np.random.seed(42) # Set random seed for reproducibility\n"
      ],
      "metadata": {
        "id": "mlp_setup_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Helper Functions**\n",
        "\n",
        "This section defines essential helper functions required for the MLP: the sigmoid activation function and its derivative, and the Mean Squared Error (MSE) cost function. Optional L1 and L2 regularization terms are also included, though they are commented out in the main training loop by default."
      ],
      "metadata": {
        "id": "mlp_helper_functions_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid activation function.\n",
        "    \n",
        "    Args:\n",
        "        z (np.array): Input to the sigmoid function (scalar, vector, or matrix).\n",
        "        \n",
        "    Returns:\n",
        "        np.array: Output of the sigmoid function, element-wise.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def MLP_sigmoid_derivative(rZ):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the sigmoid function.\n",
        "    \n",
        "    Args:\n",
        "        rZ (np.array): Input to the sigmoid function (pre-activation values).\n",
        "        \n",
        "    Returns:\n",
        "        np.array: Derivative of the sigmoid, element-wise, at rZ.\n",
        "    \"\"\"\n",
        "    sigma_of_rZ = MLP_sigmoid(rZ)\n",
        "    return sigma_of_rZ * (1.0 - sigma_of_rZ)\n",
        "\n",
        "def MLP_MSE_cost(y_true, y_pred, model, W1, W2):\n",
        "    \"\"\"\n",
        "    Computes the Mean Squared Error (MSE) cost function.\n",
        "    \n",
        "    Args:\n",
        "        y_true (np.array): True target values (1xBatchSize or BatchSize x 1).\n",
        "        y_pred (np.array): Predicted output values (1xBatchSize).\n",
        "        model (dict): Dictionary containing model parameters (e.g., regularization lambdas).\n",
        "        W1 (np.array): Weights matrix for the hidden layer.\n",
        "        W2 (np.array): Weights matrix for the output layer.\n",
        "        \n",
        "    Returns:\n",
        "        float: The scalar MSE cost.\n",
        "    \"\"\"\n",
        "    # Ensure y_true has the same shape as y_pred for element-wise operations\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape(1, -1)\n",
        "\n",
        "    cost = np.sum((y_true - y_pred)**2) / (2 * y_true.shape[1])\n",
        "    \n",
        "    # Optional: Add L1 and L2 regularization terms (commented out by default)\n",
        "    # l1 = model.get('l1', 0)\n",
        "    # l2 = model.get('l2', 0)\n",
        "    # L1_term = L1_reg(l1, W1, W2)\n",
        "    # L2_term = L2_reg(l2, W1, W2)\n",
        "    # cost = cost + L1_term + L2_term\n",
        "    \n",
        "    return cost\n",
        "\n",
        "def L2_reg(lambda_val, W1, W2):\n",
        "    \"\"\"\n",
        "    Computes the L2 regularization cost.\n",
        "    \n",
        "    Args:\n",
        "        lambda_val (float): Regularization strength.\n",
        "        W1 (np.array): Weights matrix for the hidden layer.\n",
        "        W2 (np.array): Weights matrix for the output layer.\n",
        "        \n",
        "    Returns:\n",
        "        float: The L2 regularization cost.\n",
        "    \"\"\"\n",
        "    # Exclude bias terms (first column) from regularization\n",
        "    cost = (lambda_val / 2.0) * (np.sum(W1[:, 1:]**2) + np.sum(W2[:, 1:]**2))\n",
        "    return cost\n",
        "\n",
        "def L1_reg(lambda_val, W1, W2):\n",
        "    \"\"\"\n",
        "    Computes the L1 regularization cost.\n",
        "    \n",
        "    Args:\n",
        "        lambda_val (float): Regularization strength.\n",
        "        W1 (np.array): Weights matrix for the hidden layer.\n",
        "        W2 (np.array): Weights matrix for the output layer.\n",
        "        \n",
        "    Returns:\n",
        "        float: The L1 regularization cost.\n",
        "    \"\"\"\n",
        "    # Exclude bias terms (first column) from regularization\n",
        "    cost = (lambda_val / 2.0) * (np.sum(np.abs(W1[:, 1:])) + np.sum(np.abs(W2[:, 1:])))\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "mlp_helper_functions_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. MLP Architecture Functions**\n",
        "\n",
        "This section defines the core functions for the MLP's architecture: weight initialization, forward propagation to compute predictions, backpropagation to compute gradients, and a prediction utility."
      ],
      "metadata": {
        "id": "mlp_architecture_functions_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_initialize_weights(model):\n",
        "    \"\"\"\n",
        "    Initializes weights for a two-layer MLP with values uniformly distributed in [-1, 1].\n",
        "    Bias terms are included in the weight matrices as the first column.\n",
        "    \n",
        "    Args:\n",
        "        model (dict): Dictionary containing network dimensions (n_hidden, n_features, n_output).\n",
        "        \n",
        "    Returns:\n",
        "        tuple: W1 (np.array) - Weights for hidden layer, W2 (np.array) - Weights for output layer.\n",
        "    \"\"\"\n",
        "    n_hidden = model['n_hidden']\n",
        "    n_features = model['n_features']\n",
        "    n_output = model['n_output']\n",
        "    \n",
        "    # W1: (n_hidden x (n_features + 1)) matrix, including bias for hidden layer\n",
        "    # The +1 accounts for the bias term which will be multiplied by a constant 1 in the input A0\n",
        "    W1 = 2 * np.random.rand(n_hidden, n_features + 1) - 1\n",
        "    \n",
        "    # W2: (n_output x (n_hidden + 1)) matrix, including bias for output layer\n",
        "    # The +1 accounts for the bias term which will be multiplied by a constant 1 in the activated hidden layer A1\n",
        "    W2 = 2 * np.random.rand(n_output, n_hidden + 1) - 1\n",
        "    \n",
        "    return W1, W2\n",
        "\n",
        "def MLP_MSELIN_forward(P_input, W1, W2):\n",
        "    \"\"\"\n",
        "    Performs the forward pass through the MLP.\n",
        "    \n",
        "    Args:\n",
        "        P_input (np.array): Input patterns, where each row is an observation (BatchSize x n_features).\n",
        "        W1 (np.array): Weights matrix for the hidden layer (n_hidden x (n_features + 1)).\n",
        "        W2 (np.array): Weights matrix for the output layer (n_output x (n_hidden + 1)).\n",
        "        \n",
        "    Returns:\n",
        "        tuple: rA2 (np.array) - Output layer activations (predictions),\n",
        "               A1 (np.array) - Hidden layer activations with bias,\n",
        "               A0 (np.array) - Input patterns with bias,\n",
        "               rZ1 (np.array) - Hidden layer pre-activations.\n",
        "    \"\"\"\n",
        "    batch_size = P_input.shape[0]\n",
        "    \n",
        "    # A0: Input patterns with bias term (column of ones) appended\n",
        "    # Shape: (n_features + 1) x BatchSize\n",
        "    A0 = np.vstack((np.ones(batch_size), P_input.T))\n",
        "    \n",
        "    # rZ1: Pre-activation values for the hidden layer\n",
        "    # W1: n_hidden x (n_features + 1)\n",
        "    # A0: (n_features + 1) x BatchSize\n",
        "    # rZ1: n_hidden x BatchSize\n",
        "    rZ1 = W1 @ A0\n",
        "    \n",
        "    # rA1: Activated values of the hidden layer\n",
        "    # Shape: n_hidden x BatchSize\n",
        "    rA1 = MLP_sigmoid(rZ1)\n",
        "    \n",
        "    # A1: Activated hidden layer values with bias term appended\n",
        "    # Shape: (n_hidden + 1) x BatchSize\n",
        "    A1 = np.vstack((np.ones(batch_size), rA1))\n",
        "    \n",
        "    # rA2: Output layer activations (predictions)\n",
        "    # W2: n_output x (n_hidden + 1)\n",
        "    # A1: (n_hidden + 1) x BatchSize\n",
        "    # rA2: n_output x BatchSize\n",
        "    rA2 = W2 @ A1\n",
        "    \n",
        "    return rA2, A1, A0, rZ1\n",
        "\n",
        "def MLP_MSELIN_backprop(rA2, A1, A0, rZ1, Y_true, W1, W2):\n",
        "    \"\"\"\n",
        "    Computes the partial derivatives of the loss with respect to the weight matrices W2 and W1\n",
        "    using the backpropagation algorithm.\n",
        "    \n",
        "    Args:\n",
        "        rA2 (np.array): Output layer activations (predictions) (n_output x BatchSize).\n",
        "        A1 (np.array): Hidden layer activations with bias ( (n_hidden + 1) x BatchSize).\n",
        "        A0 (np.array): Input patterns with bias ( (n_features + 1) x BatchSize).\n",
        "        rZ1 (np.array): Hidden layer pre-activations (n_hidden x BatchSize).\n",
        "        Y_true (np.array): True target values (n_output x BatchSize).\n",
        "        W1 (np.array): Weights matrix for the hidden layer (n_hidden x (n_features + 1)).\n",
        "        W2 (np.array): Weights matrix for the output layer (n_output x (n_hidden + 1)).\n",
        "        \n",
        "    Returns:\n",
        "        tuple: delta_W1_unscaled (np.array) - Gradient for W1,\n",
        "               delta_W2_unscaled (np.array) - Gradient for W2.\n",
        "    \"\"\"\n",
        "    batch_size = A0.shape[1]\n",
        "\n",
        "    # Step 1: Compute dL_dZ2 (error signal at the output layer pre-activation)\n",
        "    # This is the derivative of the MSE loss with respect to the output layer's pre-activation Z2.\n",
        "    # For MSE and linear output, dL/dZ2 = (rA2 - Y_true)\n",
        "    # Shape: n_output x BatchSize\n",
        "    dL_dZ2 = rA2 - Y_true\n",
        "    \n",
        "    # Step 2: Compute dL_dW2 (gradient for W2)\n",
        "    # dL/dW2 = dL/dZ2 * A1.T (matrix multiplication)\n",
        "    # W2: n_output x (n_hidden + 1)\n",
        "    # dL_dZ2: n_output x BatchSize\n",
        "    # A1.T: BatchSize x (n_hidden + 1)\n",
        "    # dL_dW2: n_output x (n_hidden + 1)\n",
        "    dL_dW2 = dL_dZ2 @ A1.T\n",
        "    \n",
        "    # Step 3: Compute dL_dA1 (error signal propagated back to hidden layer activations)\n",
        "    # dL/dA1 = W2.T * dL/dZ2 (matrix multiplication)\n",
        "    # W2.T: (n_hidden + 1) x n_output\n",
        "    # dL_dZ2: n_output x BatchSize\n",
        "    # dL_dA1: (n_hidden + 1) x BatchSize\n",
        "    dL_dA1 = W2.T @ dL_dZ2\n",
        "    \n",
        "    # Step 4: Compute dL_drZ1 (error signal at hidden layer pre-activation)\n",
        "    # This involves the derivative of the sigmoid activation function at the hidden layer.\n",
        "    # dL/drZ1 = (dL/dA1 excluding bias) * sigmoid_derivative(rZ1) (element-wise multiplication)\n",
        "    # dL_dA1[1:, :] extracts error signal for actual hidden units (excluding bias row)\n",
        "    # sigma_prime_of_rZ1: n_hidden x BatchSize\n",
        "    # dL_drZ1: n_hidden x BatchSize\n",
        "    sigma_prime_of_rZ1 = MLP_sigmoid_derivative(rZ1)\n",
        "    dL_drZ1 = dL_dA1[1:, :] * sigma_prime_of_rZ1\n",
        "    \n",
        "    # Step 5: Compute dL_dW1 (gradient for W1)\n",
        "    # dL/dW1 = dL/drZ1 * A0.T (matrix multiplication)\n",
        "    # W1: n_hidden x (n_features + 1)\n",
        "    # dL_drZ1: n_hidden x BatchSize\n",
        "    # A0.T: BatchSize x (n_features + 1)\n",
        "    # dL_dW1: n_hidden x (n_features + 1)\n",
        "    dL_dW1 = dL_drZ1 @ A0.T\n",
        "    \n",
        "    # Step 6: Apply regularization (if enabled - currently commented out)\n",
        "    # These are the unscaled gradients before applying learning rate and momentum.\n",
        "    delta_W1_unscaled = dL_dW1\n",
        "    delta_W2_unscaled = dL_dW2\n",
        "\n",
        "    # Example of regularization addition (if l1, l2 were active in model):\n",
        "    # l1 = model.get('l1', 0)\n",
        "    # l2 = model.get('l2', 0)\n",
        "    # delta_W1_unscaled[:, 1:] += W1[:, 1:] * (l1 + l2) # For L1/L2 on non-bias weights\n",
        "    # delta_W2_unscaled[:, 1:] += W2[:, 1:] * (l1 + l2) # For L1/L2 on non-bias weights\n",
        "\n",
        "    return delta_W1_unscaled, delta_W2_unscaled\n",
        "\n",
        "def MLP_MSELIN_predict(P_input, W1, W2):\n",
        "    \"\"\"\n",
        "    Predicts the outputs for given input patterns using the trained MLP.\n",
        "    \n",
        "    Args:\n",
        "        P_input (np.array): Input patterns, where each row is a distinct observation.\n",
        "        W1 (np.array): Trained weights matrix for the hidden layer.\n",
        "        W2 (np.array): Trained weights matrix for the output layer.\n",
        "        \n",
        "    Returns:\n",
        "        np.array: Predicted outputs (n_output x BatchSize).\n",
        "    \"\"\"\n",
        "    rA2, _, _, _ = MLP_MSELIN_forward(P_input, W1, W2)\n",
        "    return rA2\n"
      ],
      "metadata": {
        "id": "mlp_architecture_functions_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. MLP Training Function**\n",
        "\n",
        "The `MLP_MSELIN_train` function orchestrates the entire training process. It initializes weights, then iterates through a specified number of epochs, processing data in mini-batches. For each mini-batch, it performs a forward pass, calculates the loss, computes gradients via backpropagation, and updates the weights using Gradient Descent."
      ],
      "metadata": {
        "id": "mlp_training_function_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_MSELIN_train(P_train, y_train, model):\n",
        "    \"\"\"\n",
        "    Trains the Multi-Layer Perceptron model using mini-batch Gradient Descent.\n",
        "    \n",
        "    Args:\n",
        "        P_train (np.array): Training input patterns (N_observations x n_features).\n",
        "        y_train (np.array): True target values for training (N_observations x n_output).\n",
        "        model (dict): Dictionary containing model configuration (eta, epochs, minibatches, etc.).\n",
        "        \n",
        "    Returns:\n",
        "        tuple: model (dict) - Updated model dictionary with trained weights and cost history,\n",
        "               W1 (np.array) - Trained weights for the hidden layer,\n",
        "               W2 (np.array) - Trained weights for the output layer.\n",
        "    \"\"\"\n",
        "    # Initialize weights\n",
        "    W1, W2 = MLP_initialize_weights(model)\n",
        "    \n",
        "    eta = model['eta']\n",
        "    epochs = model['epochs']\n",
        "    minibatches = model['minibatches']\n",
        "    \n",
        "    # Initialize cost history\n",
        "    model['cost_history'] = []\n",
        "    \n",
        "    # Momentum parameters (commented out by default)\n",
        "    # delta_W1_prev = np.zeros_like(W1)\n",
        "    # delta_W2_prev = np.zeros_like(W2)\n",
        "    # alpha = model.get('alpha', 0) # Momentum coefficient\n",
        "\n",
        "    num_observations = P_train.shape[0]\n",
        "    \n",
        "    for e in range(1, epochs + 1):\n",
        "        # Optional: Adaptive learning rate (commented out)\n",
        "        # decrease_const = model.get('decrease_const', 0)\n",
        "        # current_eta = eta / (1 + decrease_const * e)\n",
        "        current_eta = eta # Use fixed learning rate if adaptive is off\n",
        "\n",
        "        # Shuffle data for each epoch\n",
        "        shuffled_indices = np.random.permutation(num_observations)\n",
        "        P_shuffled = P_train[shuffled_indices, :]\n",
        "        y_shuffled = y_train[shuffled_indices, :]\n",
        "\n",
        "        # Reshape indices for mini-batching\n",
        "        # np.array_split handles cases where num_observations is not perfectly divisible by minibatches\n",
        "        mini_batch_indices = np.array_split(np.arange(num_observations), minibatches)\n",
        "                          \n",
        "        for m_idx, idx in enumerate(mini_batch_indices):\n",
        "            # Extract mini-batch data\n",
        "            P_mini_batch = P_shuffled[idx, :]\n",
        "            y_mini_batch = y_shuffled[idx, :].T # Transpose y_mini_batch to match expected 1xBatchSize or n_output x BatchSize\n",
        "            \n",
        "            # Forward pass\n",
        "            rA2, A1, A0, rZ1 = MLP_MSELIN_forward(P_mini_batch, W1, W2)\n",
        "            \n",
        "            # Compute cost\n",
        "            cost = MLP_MSE_cost(y_mini_batch, rA2, model, W1, W2)\n",
        "            model['cost_history'].append(cost)\n",
        "            \n",
        "            # Print progress\n",
        "            print(f'Epoch {e}/{epochs}, Minibatch {m_idx+1}/{minibatches}, Loss (MSE) {cost:.6f}')\n",
        "            \n",
        "            # Compute gradient via backpropagation\n",
        "            delta_W1_unscaled, delta_W2_unscaled = MLP_MSELIN_backprop(rA2, A1, A0, rZ1, y_mini_batch, W1, W2)\n",
        "            \n",
        "            # Update parameters using Gradient Descent\n",
        "            delta_W1 = current_eta * delta_W1_unscaled\n",
        "            delta_W2 = current_eta * delta_W2_unscaled\n",
        "            \n",
        "            W1 = W1 - delta_W1\n",
        "            W2 = W2 - delta_W2\n",
        "            \n",
        "            # Optional: Apply momentum (commented out)\n",
        "            # W1 = W1 - (delta_W1 + (alpha * delta_W1_prev))\n",
        "            # W2 = W2 - (delta_W2 + (alpha * delta_W2_prev))\n",
        "            # delta_W1_prev = delta_W1\n",
        "            # delta_W2_prev = delta_W2\n",
        "            \n",
        "    model['W1'] = W1\n",
        "    model['W2'] = W2\n",
        "    return model, W1, W2\n"
      ],
      "metadata": {
        "id": "mlp_training_function_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model Definition and Training Example**\n",
        "\n",
        "This section sets up the MLP model parameters, generates a synthetic dataset for a simple regression problem, trains the MLP using the `MLP_MSELIN_train` function, and visualizes the training loss and the model's predictions against the true data."
      ],
      "metadata": {
        "id": "mlp_example_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "model = {\n",
        "    'n_features': 1,   # Number of input features\n",
        "    'n_hidden': 10,    # Number of neurons in the hidden layer\n",
        "    'n_output': 1,     # Number of output neurons\n",
        "    'eta': 0.01,       # Learning rate\n",
        "    'epochs': 500,     # Number of training epochs\n",
        "    'minibatches': 10, # Number of mini-batches per epoch\n",
        "    # 'alpha': 0.9,    # Momentum coefficient (uncomment if using momentum)\n",
        "    # 'l1': 0.001,     # L1 regularization strength (uncomment for L1)\n",
        "    # 'l2': 0.001      # L2 regularization strength (uncomment for L2)\n",
        "}\n",
        "\n",
        "# Generate synthetic data for a regression problem\n",
        "num_samples = 200\n",
        "X_train = np.linspace(-5, 5, num_samples).reshape(-1, 1) # Input feature\n",
        "y_train = np.sin(X_train) + 0.2 * np.random.randn(num_samples, 1) # True output with noise\n",
        "\n",
        "# Train the MLP model\n",
        "print(\"\\nStarting MLP training...\")\n",
        "trained_model, W1_trained, W2_trained = MLP_MSELIN_train(X_train, y_train, model)\n",
        "print(\"MLP training complete.\")\n",
        "\n",
        "# Plot the training loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(trained_model['cost_history'], label='Training Loss (MSE)', color='blue')\n",
        "plt.xlabel('Mini-batch Iteration')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('MLP Training Loss Over Iterations')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the training data\n",
        "y_pred_train = MLP_MSELIN_predict(X_train, W1_trained, W2_trained).T # Transpose to match y_train shape\n",
        "\n",
        "# Plot original data and MLP predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, label='True Data', color='gray', alpha=0.7, s=20)\n",
        "plt.plot(X_train, y_pred_train, label='MLP Predictions', color='red', linewidth=2)\n",
        "plt.xlabel('Input Feature')\n",
        "plt.ylabel('Output Value')\n",
        "plt.title('MLP Regression: True vs. Predicted Values')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mlp_example_code"
      }
    }
  ]
}
