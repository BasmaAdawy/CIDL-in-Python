{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Least Squares using Gradient Descent**\n",
        "\n",
        "This notebook demonstrates the implementation of Linear Least Squares (LLS) using the Gradient Descent optimization algorithm. The objective is to find the optimal parameters (weights and bias) for a linear model that best fits a given dataset by minimizing the sum of squared errors."
      ],
      "metadata": {
        "id": "lls_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Theoretical Background**\n",
        "\n",
        "$$ \\hat{y}_i = x_i \\cdot w + b $$\n",
        "\n",
        "Substituting the linear model into the error function, we get:\n",
        "\n",
        "$$E_i = \\frac{1}{2}(y_i - (x_i \\cdot w + b))^2 $$\n",
        "\n",
        "To minimize this error using Gradient Descent, we need to compute the partial derivatives of $E_i$ with respect to the bias ($b$) and each weight ($w^j$).\n",
        "\n",
        "The derivative of $E_i$ with respect to $b$ is:\n",
        "\n",
        "$$ \\frac{\\delta E_i}{\\delta b} = -(y_i - (x_i \\cdot w + b)) = (\\hat{y}_i - y_i)$$\n",
        "\n",
        "The derivative of $E_i$ with respect to each weight $w^j$ (corresponding to feature $x^j_i$) is:\n",
        "\n",
        "$$\\frac{\\delta E_i}{\\delta w^j} = -(y_i - (x_i \\cdot w + b)) \\cdot x^j_i = (\\hat{y}_i - y_i) \\cdot x^j_i $$\n",
        "\n",
        "In the Gradient Descent update rule, we move in the opposite direction of the gradient. Thus, the updates for parameters will be:\n",
        "\n",
        "$$ w \\leftarrow w - \\eta \\frac{\\delta E_i}{\\delta w^j} = w - \\eta (\\hat{y}_i - y_i) \\cdot x^j_i $$\n",
        "$$ b \\leftarrow b - \\eta \\frac{\\delta E_i}{\\delta b} = b - \\eta (\\hat{y}_i - y_i) $$\n",
        "\n",
        "where $\\eta$ is the learning rate."
      ],
      "metadata": {
        "id": "lls_theory_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D # Included for consistency, though not directly used in 1D/2D LLS\n",
        "\n",
        "np.random.seed(42) # Set random seed for reproducibility\n"
      ],
      "metadata": {
        "id": "lls_setup_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Parameters and Data Generation**\n",
        "\n",
        "This section defines the key parameters for the simulation, including the number of input features, observations, learning rate, and iterations. It then generates a synthetic dataset based on a true linear relationship with added noise, which will be used for training the LLS model."
      ],
      "metadata": {
        "id": "lls_data_gen_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1      # Number of input features (dimensions of x). Currently set for 1D input.\n",
        "m = 100    # Number of observations (data points)\n",
        "eta = 0.01 # Learning rate for Gradient Descent\n",
        "num_iters = 100 # Number of iterations (epochs) for Gradient Descent\n",
        "noise_level = 0.1 # Standard deviation of Gaussian noise added to the true y values\n",
        "\n",
        "# Generate random input data X. Each row is an observation, each column is a feature.\n",
        "X = np.random.rand(m, n)\n",
        "\n",
        "# Define the 'true' underlying weights (w_true) and bias (b_true)\n",
        "# Corrected: Define b_true and w_true with positive, sequential values for clarity\n",
        "b_true = 1.0 # True bias term\n",
        "w_true = np.arange(1, n + 1).reshape(-1, 1) # True weights for features: [1, 2, ..., n]\n",
        "\n",
        "# Combine b_true and w_true into a single vector for the design matrix multiplication\n",
        "w_true_full = np.vstack((np.array([[b_true]]), w_true))\n",
        "\n",
        "print(f\"True bias (b_true): {b_true:.4f}\")\n",
        "print(f\"True weights (w_true): {w_true.T}\")\n",
        "\n",
        "# Construct the design matrix A by adding a column of ones for the bias term\n",
        "A = np.hstack((np.ones((m, 1)), X))\n",
        "\n",
        "# Compute the 'true' y values using the linear model and add Gaussian noise\n",
        "y = A @ w_true_full + noise_level * np.random.randn(m, 1)\n"
      ],
      "metadata": {
        "id": "lls_data_gen_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Linear Least Squares by Gradient Descent Function**\n",
        "\n",
        "This function `LLS_by_GD` implements the core Gradient Descent algorithm for Linear Least Squares. It iteratively updates the model parameters (weights `w` and bias `b`) by moving in the direction opposite to the gradient of the error function. The process continues for a specified number of iterations, and the average error at each iteration is recorded."
      ],
      "metadata": {
        "id": "lls_function_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LLS_by_GD(X, y, eta, num_iters):\n",
        "    \"\"\"\n",
        "    Performs Linear Least Squares regression using Gradient Descent.\n",
        "\n",
        "    Args:\n",
        "        X (np.array): Input features (N_observations, N_features).\n",
        "        y (np.array): True output values (N_observations, 1).\n",
        "        eta (float): The learning rate.\n",
        "        num_iters (int): The number of iterations (epochs) to perform.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - w (np.array): Learned weights (N_features, 1).\n",
        "            - b (float): Learned bias.\n",
        "            - avg_E_at_each_iteration (np.array): History of average errors per iteration.\n",
        "    \"\"\"\n",
        "    N, D = X.shape # N: number of observations, D: number of features\n",
        "\n",
        "    # Initialize weights (w) and bias (b) to zeros\n",
        "    w = np.zeros((D, 1)) # Weights as a column vector\n",
        "    b = 0.0              # Bias as a scalar\n",
        "\n",
        "    # Initialize array to store the average error at each iteration\n",
        "    avg_E_at_each_iteration = np.zeros(num_iters)\n",
        "\n",
        "    # Gradient Descent main loop (iterates over epochs)\n",
        "    for k in range(num_iters):\n",
        "        # Initialize total error for the current epoch\n",
        "        E_current_epoch = 0.0\n",
        "\n",
        "        # Iterate over each training example (Stochastic Gradient Descent update)\n",
        "        for i in range(N):\n",
        "            # Predict the output for the current example using the current w and b\n",
        "            # X[i, :].reshape(1, -1) ensures X[i,:] is a row vector for dot product\n",
        "            y_pred = X[i, :].reshape(1, -1) @ w + b\n",
        "            \n",
        "            # Calculate the residual (prediction error: predicted - actual)\n",
        "            residual = y_pred - y[i] # This is a scalar value\n",
        "            \n",
        "            # Accumulate the squared error for this example for epoch error calculation\n",
        "            E_current_epoch += 0.5 * residual**2\n",
        "            \n",
        "            # Compute the gradients for weights and bias\n",
        "            # gradient_w: (residual * X[i,:]) should be (D, 1) to match w's shape\n",
        "            gradient_w = residual * X[i, :].reshape(-1, 1)\n",
        "            # gradient_b: (residual) is scalar\n",
        "            gradient_b = residual\n",
        "            \n",
        "            # Update parameters (w and b) using the Gradient Descent rule\n",
        "            # Parameters are moved in the opposite direction of the gradient\n",
        "            w = w - eta * gradient_w\n",
        "            b = b - eta * gradient_b\n",
        "\n",
        "        # Store the average error for the current epoch\n",
        "        avg_E_at_each_iteration[k] = E_current_epoch / N\n",
        "\n",
        "    return w, b, avg_E_at_each_iteration\n"
      ],
      "metadata": {
        "id": "lls_function_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model Training and Visualization**\n",
        "\n",
        "This section executes the `LLS_by_GD` function to train the linear model on the generated data. It then visualizes the results by plotting the original data points along with the learned regression line (for 1D input) or plane (for 2D input). Finally, it displays the learned weights and bias, and plots the evolution of the average error over the training iterations, demonstrating the convergence of the Gradient Descent algorithm."
      ],
      "metadata": {
        "id": "lls_execution_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a linear model to the data using Gradient Descent\n",
        "w_learned, b_learned, E_history = LLS_by_GD(X, y, eta, num_iters)\n",
        "\n",
        "# Plotting the data and the fitted hyperplane/line based on the number of features (n)\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "if n == 1:\n",
        "    # For 1D input, plot scatter points and the regression line\n",
        "    plt.scatter(X[:, 0], y, label='Original Data', color='blue', alpha=0.7)\n",
        "    \n",
        "    # Generate points for the fitted line\n",
        "    x_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "    y_line = x_line @ w_learned + b_learned\n",
        "    plt.plot(x_line, y_line, color='red', linewidth=2, label='Fitted Regression Line')\n",
        "    \n",
        "    plt.xlabel('Feature X')\n",
        "    plt.ylabel('Output Y')\n",
        "    plt.title('Linear Regression with Gradient Descent (1D Input)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "elif n == 2:\n",
        "    # For 2D input, a 3D plot would be required (e.g., scatter3 and surf)\n",
        "    # This part is a placeholder as per the original MATLAB script's 'error TBC'\n",
        "    print(\"Plotting for n=2 is not implemented in this version (requires 3D visualization).\")\n",
        "    # Example structure for 3D plot (requires more detailed implementation):\n",
        "    # fig = plt.figure()\n",
        "    # ax = fig.add_subplot(111, projection='3d')\n",
        "    # ax.scatter(X[:, 0], X[:, 1], y, label='Original Data')\n",
        "    # ... define grid for surface and plot ...\n",
        "    # plt.show()\n",
        "\n",
        "else:\n",
        "    # For higher dimensions, visualization is not straightforward\n",
        "    print(f\"Visualization for n={n} features is not supported in this script.\")\n",
        "\n",
        "# Print the learned weights and bias\n",
        "print(f\"\\nLearned weights (w): {w_learned.T}\")\n",
        "print(f\"Learned bias (b): {b_learned:.4f}\")\n",
        "\n",
        "# Plot the cost function (average error) values along the iterations\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_iters + 1), E_history, color='green', linewidth=2)\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Average Error (E)')\n",
        "plt.title('Average Error vs. Iteration in Gradient Descent')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lls_execution_code"
      }
    }
  ]
}
