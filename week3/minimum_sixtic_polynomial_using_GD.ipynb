{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Minimizing a Sixtic Polynomial using Gradient Descent**\n",
        "\n",
        "This notebook demonstrates the application of the Gradient Descent algorithm to find the local minima of a sixth-degree polynomial function."
      ],
      "metadata": {
        "id": "intro_gd_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# mpl_toolkits.mplot3d is not directly used in this script but is included\n",
        "# for consistency with previous notebooks if 3D plotting were to be added.\n",
        "from mpl_toolkits.mplot3d import Axes3D \n",
        "\n",
        "np.random.seed(42) # For reproducibility of random processes\n"
      ],
      "metadata": {
        "id": "gd_setup_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Parameters**\n",
        "\n",
        "Define the learning rate (`eta`), the total number of iterations, and the frequency for displaying iteration progress for the Gradient Descent algorithm."
      ],
      "metadata": {
        "id": "gd_parameters_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.0005  # Learning rate: controls the step size in each iteration\n",
        "num_iters = 1000 # Total number of iterations for gradient descent\n",
        "num_iter_to_display = 50 # Number of initial and final iterations to print\n"
      ],
      "metadata": {
        "id": "gd_parameters_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Sixtic Polynomial Definition**\n",
        "\n",
        "Define the coefficients of the sixth-degree polynomial function. The polynomial is given by:\n",
        "\n",
        "$$ P(x) = \\frac{1}{10} (x^6 -4x^5 -26x^4 +56x^3 + 253x^2 + 20x -300) $$\n",
        "\n",
        "This polynomial has six real roots: $[-3, -2, -2, 1, 5, 5]$.\n",
        "In NumPy, a polynomial is typically represented by a `poly1d` object, which simplifies evaluation and differentiation."
      ],
      "metadata": {
        "id": "gd_polynomial_definition_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficients of the sixtic polynomial, from highest degree to constant term\n",
        "sixtic_pol_coeffs = 1/10 * np.array([1, -4, -26, 56, 253, 20, -300])\n",
        "\n",
        "# Create a polynomial object using np.poly1d for easy evaluation and differentiation\n",
        "P = np.poly1d(sixtic_pol_coeffs)\n",
        "\n",
        "# Generate x values for plotting the polynomial function\n",
        "m = 1000\n",
        "x_range = np.linspace(-7, 8.5, m)\n",
        "\n",
        "# Evaluate the polynomial function over the x_range\n",
        "y_values = P(x_range)\n"
      ],
      "metadata": {
        "id": "gd_polynomial_definition_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Polynomial Visualization**\n",
        "\n",
        "Visualize the sixtic polynomial function over a broad range and then a zoomed-in view. The zoomed-in plot helps to highlight the local minima and maxima that Gradient Descent will attempt to find."
      ],
      "metadata": {
        "id": "gd_polynomial_visualization_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_range, y_values, linewidth=2)\n",
        "plt.grid(True) # Add a grid for better readability\n",
        "plt.title('Sixtic Polynomial Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('P(x)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_range, y_values, linewidth=2)\n",
        "plt.grid(True)\n",
        "plt.xlim([-6, 8])  # Set x-axis limits for zoomed view\n",
        "plt.ylim([-50, 140]) # Set y-axis limits for zoomed view\n",
        "plt.title('Sixtic Polynomial Function (Zoomed In)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('P(x)')\n",
        "\n",
        "# Add annotations for axes (approximating MATLAB's annotation function)\n",
        "# x-axis arrow\n",
        "plt.annotate('', xy=(plt.xlim()[1], 0), xytext=(plt.xlim()[0], 0), \n",
        "             arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8))\n",
        "plt.text(plt.xlim()[1] + 0.2, 0, 'x', fontsize=12, ha='left', va='center')\n",
        "\n",
        "# y-axis arrow\n",
        "plt.annotate('', xy=(0, plt.ylim()[1]), xytext=(0, plt.ylim()[0]), \n",
        "             arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8))\n",
        "plt.text(0, plt.ylim()[1] + 5, 'P(x)', fontsize=12, ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gd_polynomial_visualization_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Gradient Calculation**\n",
        "\n",
        "Compute the first derivative (i.e., the gradient) of the polynomial function. In NumPy, this can be easily done using the `.deriv()` method of a `poly1d` object. The derivative is crucial for Gradient Descent as it indicates the direction of the steepest ascent."
      ],
      "metadata": {
        "id": "gd_gradient_calculation_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the first derivative of the polynomial P\n",
        "grad_P = P.deriv()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_range, y_values, linewidth=2, label='Sixtic Polynomial Function')\n",
        "plt.grid(True)\n",
        "# Plot the derivative (gradient) function\n",
        "plt.plot(x_range, grad_P(x_range), '-r', linewidth=2, label='Its Derivative (Gradient)')\n",
        "plt.legend(loc='upper left', fontsize=12) # Add a legend to distinguish plots\n",
        "plt.xlim([-5.5814, 7.2100]) # Set x-axis limits\n",
        "plt.ylim([-650.015, 1015.517]) # Set y-axis limits\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Polynomial Function and Its Derivative')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gd_gradient_calculation_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Critical Points of the Polynomial**\n",
        "\n",
        "The roots of the gradient function correspond to the x-coordinates where the original polynomial has local minima or maxima. These are the critical points where the slope is zero."
      ],
      "metadata": {
        "id": "gd_critical_points_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the roots of the derivative (gradient) polynomial\n",
        "location_of_local_minima_and_maxima = grad_P.roots\n",
        "print(f\"Locations of local minima and maxima (roots of the gradient): {location_of_local_minima_and_maxima}\")\n",
        "\n",
        "# For this specific polynomial, the approximate location of the true absolute minimum is at x = -0.04007195,\n",
        "# where the function value is y = -30.03988514.\n"
      ],
      "metadata": {
        "id": "gd_critical_points_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Gradient Descent Algorithm Implementation**\n",
        "\n",
        "This function implements the core Gradient Descent algorithm. It iteratively updates the current 'x' value by moving in the direction opposite to the gradient, scaled by the learning rate, aiming to reach a local minimum of the polynomial function."
      ],
      "metadata": {
        "id": "gd_algorithm_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimization_using_GD(pol, x0, grad, eta, num_iters):\n",
        "    \"\"\"\n",
        "    Performs minimization of a polynomial function using Gradient Descent.\n",
        "\n",
        "    Args:\n",
        "        pol (np.poly1d): The polynomial function to minimize.\n",
        "        x0 (float): The initial starting point for x.\n",
        "        grad (np.poly1d): The derivative (gradient) of the polynomial function.\n",
        "        eta (float): The learning rate.\n",
        "        num_iters (int): The number of iterations to perform.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - x_history (np.array): History of x values during optimization.\n",
        "            - pol_history (np.array): History of polynomial values at x_history.\n",
        "    \"\"\"\n",
        "    # Initialize arrays to store the history of x values and corresponding polynomial values\n",
        "    x_history = np.zeros(num_iters + 1)\n",
        "    pol_history = np.zeros(num_iters + 1)\n",
        "\n",
        "    # Store the initial values\n",
        "    x_history[0] = x0\n",
        "    pol_history[0] = pol(x0)\n",
        "\n",
        "    old_x = x0\n",
        "\n",
        "    # Gradient Descent loop\n",
        "    for k in range(num_iters):\n",
        "        # Calculate the new x value using the gradient descent update rule\n",
        "        # new_x = old_x - learning_rate * gradient(old_x)\n",
        "        new_x = old_x - eta * grad(old_x)\n",
        "        \n",
        "        # Update old_x for the next iteration\n",
        "        old_x = new_x\n",
        "        \n",
        "        # Store the current x value and its corresponding polynomial value\n",
        "        x_history[k + 1] = new_x\n",
        "        pol_history[k + 1] = pol(new_x)\n",
        "\n",
        "    return x_history, pol_history\n"
      ],
      "metadata": {
        "id": "gd_algorithm_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Displaying Iterations**\n",
        "\n",
        "This helper function prints a summary of the Gradient Descent iterations, showing the initial, a few intermediate, and the final steps of the optimization process."
      ],
      "metadata": {
        "id": "gd_display_iterations_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displayIterations(x_his, pol_his, num_iter_to_display, x0):\n",
        "    \"\"\"\n",
        "    Displays a summary of the Gradient Descent iterations.\n",
        "\n",
        "    Args:\n",
        "        x_his (np.array): History of x values.\n",
        "        pol_his (np.array): History of polynomial values.\n",
        "        num_iter_to_display (int): Number of initial and final iterations to display.\n",
        "        x0 (float): The initial starting point for x.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\\nDisplaying the first {num_iter_to_display} and the latest {num_iter_to_display} iterations\\nwhen starting from x0={x0}\")\n",
        "    \n",
        "    # Display initial iterations\n",
        "    for i in range(min(num_iter_to_display + 1, len(x_his))):\n",
        "        print(f\"it:{i:<3d}  x:{x_his[i]:<10.5f}  pol(x):{pol_his[i]:<10.5f}\")\n",
        "    \n",
        "    # Display ellipsis if there are more iterations than displayed\n",
        "    if len(x_his) > 2 * num_iter_to_display + 1:\n",
        "        print('...')\n",
        "\n",
        "    # Display latest iterations\n",
        "    for i in range(max(num_iter_to_display + 1, len(x_his) - num_iter_to_display), len(x_his)):\n",
        "        print(f\"it:{i:<4d}  x:{x_his[i]:<10.5f}  pol(x):{pol_his[i]:<10.5f}\")\n"
      ],
      "metadata": {
        "id": "gd_display_iterations_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Running Gradient Descent from Different Starting Points**\n",
        "\n",
        "Execute the Gradient Descent algorithm with various initial 'x' values (`x0`) to demonstrate its convergence behavior. Gradient Descent converges to the local minimum closest to the starting point, not necessarily the global minimum, depending on the function's landscape."
      ],
      "metadata": {
        "id": "gd_running_gd_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting from x0 = -6: Expected to converge to the first local minimum from the left (approx. x = -2.70)\n",
        "x0_1 = -6\n",
        "x_his_1, pol_his_1 = minimization_using_GD(P, x0_1, grad_P, eta, num_iters)\n",
        "displayIterations(x_his_1, pol_his_1, num_iter_to_display, x0_1)\n",
        "\n",
        "# Starting from x0 = +8: Expected to converge to the local minimum on the right (approx. x = +5)\n",
        "x0_2 = +8\n",
        "x_his_2, pol_his_2 = minimization_using_GD(P, x0_2, grad_P, eta, num_iters)\n",
        "displayIterations(x_his_2, pol_his_2, num_iter_to_display, x0_2)\n",
        "\n",
        "# Starting from x0 = +2: Expected to converge to the true absolute minimum (approx. x = -0.04)\n",
        "x0_3 = +2\n",
        "x_his_3, pol_his_3 = minimization_using_GD(P, x0_3, grad_P, eta, num_iters)\n",
        "displayIterations(x_his_3, pol_his_3, num_iter_to_display, x0_3)\n"
      ],
      "metadata": {
        "id": "gd_running_gd_code"
      }
    }
  ]
}
