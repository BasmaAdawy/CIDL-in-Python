{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Layer Perceptron (MLP) for MNIST Digit Recognition**\n",
        "\n",
        "This notebook provides a complete implementation of a Multi-Layer Perceptron (MLP) for the MNIST handwritten digit classification problem. It focuses on demonstrating the fundamental mechanics of a neural network from scratch using `numpy`, without relying on high-level deep learning frameworks like PyTorch or TensorFlow for the MLP's core logic. The dataset loading, however, leverages `torchvision` for convenience in a Colab environment, and then converts the data to NumPy arrays for further processing.\n",
        "\n",
        "The notebook covers:\n",
        "\n",
        "1.  **Theoretical Background:** A concise overview of MLP architecture, activation functions (Sigmoid, Softmax), Categorical Cross-Entropy (CCE) loss, regularization (L1, L2), forward pass, and backpropagation.\n",
        "2.  **MNIST Data Handling:** Functions for loading and visualizing the MNIST dataset.\n",
        "3.  **MLP Core Functions:** Detailed NumPy implementations of sigmoid, softmax, weight initialization, CCE loss calculation, forward propagation, backpropagation with gradient computations, and prediction.\n",
        "4.  **MLP Training Loop:** The main training function incorporating mini-batch gradient descent with momentum and an adaptive learning rate.\n",
        "5.  **Training and Evaluation:** Script to train the MLP on MNIST, evaluate its accuracy on training and test sets, and visualize the training loss.\n",
        "6.  **Performance Visualization:** Displaying confusion matrices and sample predictions to assess model performance."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D # Imported for consistency, though not directly used in 2D MNIST visualization.\n",
        "\n",
        "np.random.seed(42)  # Set random seed for reproducibility of initial weights and data shuffling."
      ],
      "metadata": {
        "id": "setup_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Theoretical Background**\n",
        "\n",
        "## **1.1 Multi-Layer Perceptron (MLP) Architecture**\n",
        "An MLP is a class of feedforward artificial neural network. It consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer. Each node (neuron) in a layer is connected to every node in the subsequent layer, forming a densely connected or fully connected network. Information flows in one direction, from input to output, without loops. For this classification task, a two-layer MLP (one hidden layer) is employed.\n",
        "\n",
        "## **1.2 Activation Functions**\n",
        "Activation functions introduce non-linearity into the network, enabling it to learn complex, non-linear relationships in the data. Without them, an MLP would simply behave as a linear model.\n",
        "\n",
        "* **Sigmoid Activation (Hidden Layer):** The sigmoid function maps any real-valued number to a value between 0 and 1. It's often used in hidden layers due to its non-linear and differentiable properties.\n",
        "    $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "* **Sigmoid Derivative:** The derivative of the sigmoid function is crucial for the backpropagation algorithm, as it determines the gradient for updating weights.\n",
        "    $$ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $$\n",
        "* **Softmax Activation (Output Layer):** The softmax function is used in the output layer for multi-class classification problems. It converts a vector of arbitrary real values into a probability distribution, where the sum of probabilities for all classes equals 1. This makes the output directly interpretable as class probabilities.\n",
        "    $$ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for a K-class problem} $$\n",
        "\n",
        "## **1.3 Cost Function: Categorical Cross-Entropy (CCE)**\n",
        "The cost (or loss) function quantifies the discrepancy between the network's predicted outputs and the true labels. The goal of training is to minimize this cost. For multi-class classification with softmax output and one-hot encoded true labels, Categorical Cross-Entropy (CCE) is the standard choice.\n",
        "\n",
        "For a single training sample $p$ and across all $C$ output classes, the CCE loss is given by:\n",
        "$$ L(y_{true}, \\hat{y}) = -\\sum_{c=1}^{C} y_{true,c} \\, \\log(\\hat{y}_c) $$\n",
        "where $y_{true,c}$ is 1 if the true class is $c$ and 0 otherwise (one-hot encoding), and $\\hat{y}_c$ is the predicted probability for class $c$. The total cost for a batch is the sum of individual sample costs.\n",
        "\n",
        "## **1.4 Regularization**\n",
        "Regularization techniques are applied to prevent overfitting, a phenomenon where the model performs well on training data but poorly on unseen data. They add a penalty term to the cost function based on the magnitude of the weights.\n",
        "\n",
        "* **L1 Regularization (Lasso):** Adds a penalty proportional to the absolute value of the weights. This encourages sparsity, potentially driving some weights to exactly zero.\n",
        "    $$ R_{L1}(\\mathbf{W}) = \\frac{\\lambda_1}{2} \\sum_{j} |w_j| $$\n",
        "* **L2 Regularization (Ridge):** Adds a penalty proportional to the square of the weights. This discourages large weights, promoting a more distributed and less complex model.\n",
        "    $$ R_{L2}(\\mathbf{W}) = \\frac{\\lambda_2}{2} \\sum_{j} w_j^2 $$\n",
        "\n",
        "## **1.5 Forward Pass**\n",
        "The forward pass is the process of computing the network's output predictions ($\\hat{Y}$) for a given input ($X$) by propagating the input through each layer.\n",
        "\n",
        "1.  **Input Layer to Hidden Layer:**\n",
        "    * Augment the input $X$ with a bias term to form $A_0$ (input activations). If $X$ is $(N_{batch}, N_{features})$, then $A_0$ is $(N_{batch}, 1 + N_{features})$.\n",
        "    * Compute the pre-activation values for the hidden layer: $Z_1 = A_0 W_1^T$. $W_1$ is the weight matrix for the hidden layer, with shape $(N_{hidden}, 1 + N_{features})$.\n",
        "    * Apply the sigmoid activation function to $Z_1$ to get $A_1$ (hidden layer activations).\n",
        "    * Augment $A_1$ with a bias term to form $A_1^{ext}$. $A_1^{ext}$ has shape $(N_{batch}, 1 + N_{hidden})$.\n",
        "\n",
        "2.  **Hidden Layer to Output Layer:**\n",
        "    * Compute the pre-activation values for the output layer: $Z_2 = A_1^{ext} W_2^T$. $W_2$ is the weight matrix for the output layer, with shape $(N_{output}, 1 + N_{hidden})$.\n",
        "    * Apply the softmax activation function to $Z_2$ to get $\\hat{Y}$ (the final predicted probabilities).\n",
        "\n",
        "## **1.6 Backpropagation**\n",
        "Backpropagation is the algorithm used to efficiently compute the gradients of the cost function with respect to each weight in the network. These gradients are then used to update the weights during training.\n",
        "\n",
        "1.  **Output Layer Error ($\\delta_2$):** For Categorical Cross-Entropy with Softmax output, the error signal at the output layer's pre-activation is remarkably simple:\n",
        "    $$ \\delta_2 = \\hat{Y} - Y_{true} \\quad \\text{(where } \\hat{Y} \\text{ are softmax outputs and } Y_{true} \\text{ are one-hot encoded labels)} $$\n",
        "    If $\\hat{Y}$ and $Y_{true}$ are $(N_{batch}, N_{output})$, then $\\delta_2$ is also $(N_{batch}, N_{output})$.\n",
        "\n",
        "2.  **Gradient for Output Weights ($W_2$):** The gradient for $W_2$ is computed as the outer product of the output error and the hidden layer activations (extended with bias).\n",
        "    $$ \\nabla_{W_2} J = \\delta_2^T A_1^{ext} $$\n",
        "    Resulting in a shape $(N_{output}, 1 + N_{hidden})$.\n",
        "\n",
        "3.  **Hidden Layer Error ($\\delta_1$):** This error signal is propagated back from the output layer to the hidden layer's pre-activations. It involves multiplying the output error by the output weights and then element-wise multiplying by the derivative of the hidden layer's activation function.\n",
        "    $$ \\delta_1 = (\\delta_2 W_2)_{\\text{excluding bias}} \\odot \\sigma'(Z_1) $$\n",
        "    The term $(\\delta_2 W_2)_{\\text{excluding bias}}$ selects only the error signals relevant to the activated neurons in the hidden layer (excluding the bias connection). Resulting in a shape $(N_{batch}, N_{hidden})$.\n",
        "\n",
        "4.  **Gradient for Hidden Weights ($W_1$):** The gradient for $W_1$ is computed as the outer product of the hidden layer error and the input activations (extended with bias).\n",
        "    $$ \\nabla_{W_1} J = \\delta_1^T A_0 $$\n",
        "    Resulting in a shape $(N_{hidden}, 1 + N_{features})$.\n",
        "\n",
        "## **1.7 Gradient Descent Update Rule with Momentum**\n",
        "Weights are updated iteratively to minimize the cost function. Gradient Descent moves weights in the direction opposite to the gradient. Momentum is added to accelerate convergence and dampen oscillations, by incorporating a fraction of the previous weight update.\n",
        "$$ W \\leftarrow W - \\eta \\nabla J + \\alpha \\Delta W_{prev} $$\n",
        "where:\n",
        "* $W$ is the current weight matrix.\n",
        "* $\\eta$ is the learning rate, controlling the step size.\n",
        "* $\\nabla J$ is the gradient of the cost function with respect to $W$.\n",
        "* $\\alpha$ is the momentum coefficient (typically between 0 and 1).\n",
        "* $\\Delta W_{prev}$ is the weight change from the previous iteration.\n",
        "\n",
        "An adaptive learning rate (e.g., decaying over epochs) is often used to fine-tune convergence, allowing larger steps initially and smaller steps as training progresses to prevent overshooting the minimum."
      ],
      "metadata": {
        "id": "theory_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. MNIST Data Handling Functions**\n",
        "\n",
        "This section provides functions for loading the MNIST dataset and visualizing its digits. The `mnist_load` function retrieves the image and label data, while `display_mnist` renders a grid of selected digits for visual inspection. These utilities are essential for preparing and understanding the dataset before training the MLP."
      ],
      "metadata": {
        "id": "mnist_data_functions_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_load(classes, Ntrain=None):\n",
        "    \"\"\"\n",
        "    Loads MNIST data for specified classes using torchvision datasets, then converts to NumPy arrays.\n",
        "    Optionally limits the number of training patterns.\n",
        "\n",
        "    Args:\n",
        "        classes (list): A list of integers representing the digits (0-9) to include.\n",
        "        Ntrain (int, optional): Maximum number of training patterns to load. If None, loads all available.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Xtrain (np.array) - Training images (N_train, 784),\n",
        "               ytrain (np.array) - Training labels (N_train,),\n",
        "               Xtest (np.array) - Test images (N_test, 784),\n",
        "               ytest (np.array) - Test labels (N_test,).\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import torchvision\n",
        "    from torchvision import datasets, transforms\n",
        "\n",
        "    # Define a transformation to convert images to tensors and normalize them.\n",
        "    # MNIST data is normalized with mean 0.1307 and std dev 0.3081 as per common practice.\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    # Load the training and test datasets. `download=True` will download if not present.\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Filter data based on specified classes and convert to NumPy arrays\n",
        "    Xtrain_list, ytrain_list = [], []\n",
        "    for img, label in train_dataset:\n",
        "        if label in classes:\n",
        "            Xtrain_list.append(img.numpy().flatten())  # Flatten 28x28 images to 784-dim vectors\n",
        "            ytrain_list.append(label)\n",
        "\n",
        "    Xtest_list, ytest_list = [], []\n",
        "    for img, label in test_dataset:\n",
        "        if label in classes:\n",
        "            Xtest_list.append(img.numpy().flatten())\n",
        "            ytest_list.append(label)\n",
        "\n",
        "    Xtrain = np.array(Xtrain_list, dtype=np.float64) # Use float64 for numerical stability\n",
        "    ytrain = np.array(ytrain_list, dtype=int)\n",
        "    Xtest = np.array(Xtest_list, dtype=np.float64)\n",
        "    ytest = np.array(ytest_list, dtype=int)\n",
        "\n",
        "    # Shuffle training data and optionally limit its size\n",
        "    shuffle_indices_train = np.random.permutation(len(ytrain))\n",
        "    Xtrain = Xtrain[shuffle_indices_train]\n",
        "    ytrain = ytrain[shuffle_indices_train]\n",
        "    if Ntrain is not None and Ntrain < len(ytrain):\n",
        "        Xtrain = Xtrain[:Ntrain]\n",
        "        ytrain = ytrain[:Ntrain]\n",
        "\n",
        "    # Shuffle test data\n",
        "    shuffle_indices_test = np.random.permutation(len(ytest))\n",
        "    Xtest = Xtest[shuffle_indices_test]\n",
        "    ytest = ytest[shuffle_indices_test]\n",
        "\n",
        "    return Xtrain, ytrain, Xtest, ytest\n",
        "\n",
        "def display_mnist(X, example_width=None):\n",
        "    \"\"\"\n",
        "    Displays 2D image data (e.g., MNIST digits) in a grid layout.\n",
        "    The images are reshaped from flattened vectors back to 2D for display.\n",
        "\n",
        "    Args:\n",
        "        X (np.array): Input image data (N_images, N_pixels_flat).\n",
        "        example_width (int, optional): Width of each square example image. If None, inferred.\n",
        "    \"\"\"\n",
        "    if example_width is None:\n",
        "        # Assume images are square, infer width from total pixels\n",
        "        example_width = int(np.round(np.sqrt(X.shape[1])))\n",
        "\n",
        "    plt.set_cmap('gray') # Set colormap to grayscale\n",
        "    m, n = X.shape # m: number of images, n: number of flattened pixels\n",
        "    example_height = n // example_width # Calculate height of each image\n",
        "\n",
        "    # Compute number of rows and columns for the display grid\n",
        "    display_rows = int(np.floor(np.sqrt(m)))\n",
        "    display_cols = int(np.ceil(m / display_rows))\n",
        "    pad = 1 # Padding between images in the display grid\n",
        "\n",
        "    # Create a blank array for the stitched display, filled with -1 (dark for normalized images)\n",
        "    display_array = -np.ones(\n",
        "        (pad + display_rows * (example_height + pad),\n",
        "         pad + display_cols * (example_width + pad))\n",
        "    )\n",
        "\n",
        "    curr_ex = 0\n",
        "    for j in range(display_rows):\n",
        "        for i in range(display_cols):\n",
        "            if curr_ex >= m:\n",
        "                break # Stop if all examples are displayed\n",
        "            \n",
        "            # Get the maximum absolute value of the current image's pixels for normalization\n",
        "            # This ensures consistent intensity mapping across images if data is not pre-normalized to 0-1\n",
        "            max_val = np.max(np.abs(X[curr_ex, :]))\n",
        "            \n",
        "            # Copy the current example image into its patch in the display_array\n",
        "            # Reshape the 1D image vector back to 2D (height, width)\n",
        "            # Normalize by max_val to scale pixel values for consistent display\n",
        "            display_array[\n",
        "                pad + j * (example_height + pad) : pad + j * (example_height + pad) + example_height,\n",
        "                pad + i * (example_width + pad) : pad + i * (example_width + pad) + example_width\n",
        "            ] = X[curr_ex, :].reshape(example_height, example_width) / max_val\n",
        "            curr_ex += 1\n",
        "        if curr_ex >= m:\n",
        "            break\n",
        "\n",
        "    plt.figure(figsize=(display_cols*example_width/28, display_rows*example_height/28)) # Adjust figure size dynamically\n",
        "    plt.imshow(display_array)\n",
        "    plt.axis('off') # Hide axes for cleaner image display\n",
        "    plt.title('Sample of MNIST Digits')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mnist_data_functions_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. MLP Core Functions**\n",
        "\n",
        "This section implements the fundamental mathematical operations that constitute the Multi-Layer Perceptron. Each function is a self-contained unit responsible for a specific aspect of the MLP's behavior, such as activation, weight management, loss computation, and gradient calculation via backpropagation."
      ],
      "metadata": {
        "id": "mlp_core_functions_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid activation function element-wise.\n",
        "    The sigmoid function squashes any real-valued number into the range (0, 1).\n",
        "    Mathematically: \\( \\sigma(z) = 1 / (1 + e^{-z}) \\)\n",
        "    Args:\n",
        "        z (np.array): Input array (pre-activation values).\n",
        "    Returns:\n",
        "        np.array: Output of the sigmoid function, with the same shape as `z`.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def MLP_sigmoid_gradient(Z):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the sigmoid function with respect to its input Z.\n",
        "    This derivative is utilized during the backpropagation process.\n",
        "    Mathematically: \\( \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) \\)\n",
        "    Args:\n",
        "        Z (np.array): Input array (pre-activation values), for which the sigmoid and its derivative are calculated.\n",
        "    Returns:\n",
        "        np.array: Derivative of the sigmoid function, with the same shape as `Z`.\n",
        "    \"\"\"\n",
        "    sig = MLP_sigmoid(Z)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def MLP_softmax(rZ):\n",
        "    \"\"\"\n",
        "    Computes the softmax activation function. This function is typically used in the output layer\n",
        "    of a multi-class classification neural network to convert raw scores (logits) into probabilities.\n",
        "    It ensures that the output values are between 0 and 1 and sum to 1 across classes for each sample.\n",
        "    Mathematically: \\( \\text{softmax}(z_i) = e^{z_i} / \\sum_{j=1}^{K} e^{z_j} \\)\n",
        "    \n",
        "    Args:\n",
        "        rZ (np.array): Input array of pre-activation values (logits). Expected shape (batch_size, n_output_classes).\n",
        "    Returns:\n",
        "        np.array: Output probabilities (batch_size, n_output_classes), where each row sums to 1.\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability to prevent overflow with large exponents\n",
        "    # Axis=1 means calculate max for each row (each sample in the batch)\n",
        "    exp_rZ = np.exp(rZ - np.max(rZ, axis=1, keepdims=True))\n",
        "    # Sum of exponents for normalization (sum across classes for each sample)\n",
        "    total = np.sum(exp_rZ, axis=1, keepdims=True)\n",
        "    rA = exp_rZ / total\n",
        "    return rA\n",
        "\n",
        "def MLP_initialize_weights(model):\n",
        "    \"\"\"\n",
        "    Initializes the weight matrices for the MLP. Weights are initialized uniformly\n",
        "    between -1 and 1. Bias terms are incorporated as the first column in each weight matrix.\n",
        "    This initialization helps in breaking symmetry and allows the network to learn diverse features.\n",
        "    Args:\n",
        "        model (dict): A dictionary containing network architecture parameters:\n",
        "                      'n_hidden' (int): Number of neurons in the hidden layer.\n",
        "                      'n_features' (int): Number of input features.\n",
        "                      'n_output' (int): Number of neurons in the output layer (number of classes).\n",
        "    Returns:\n",
        "        tuple: W1 (np.array) - Weight matrix for the hidden layer, shape (n_hidden, n_features + 1).\n",
        "               W2 (np.array) - Weight matrix for the output layer, shape (n_output, n_hidden + 1).\n",
        "    \"\"\"\n",
        "    n_hidden = model['n_hidden']\n",
        "    n_features = model['n_features']\n",
        "    n_output = model['n_output']\n",
        "    \n",
        "    # W1: Weights connecting input layer (n_features + 1 for bias) to hidden layer (n_hidden).\n",
        "    # Dimensions are (n_hidden, n_features + 1) because W1 will be multiplied by A0 @ W1.T.\n",
        "    # Initialized in range [-1, 1].\n",
        "    W1 = 2 * np.random.rand(n_hidden, n_features + 1) - 1\n",
        "    \n",
        "    # W2: Weights connecting hidden layer (n_hidden + 1 for bias) to output layer (n_output).\n",
        "    # Dimensions are (n_output, n_hidden + 1).\n",
        "    W2 = 2 * np.random.rand(n_output, n_hidden + 1) - 1\n",
        "    return W1, W2\n",
        "\n",
        "def encode_labels(y, k):\n",
        "    \"\"\"\n",
        "    Performs one-hot encoding on a vector of integer class labels.\n",
        "    One-hot encoding converts categorical labels into a binary vector representation,\n",
        "    which is suitable for training with categorical cross-entropy loss.\n",
        "    Args:\n",
        "        y (np.array): A 1D NumPy array of original class labels (e.g., [0, 1, 2, 1, ...]).\n",
        "        k (int): The total number of unique classes (e.g., 10 for MNIST digits 0-9).\n",
        "    Returns:\n",
        "        np.array: A 2D NumPy array of one-hot encoded labels, with shape (N_samples, k).\n",
        "                  Each row corresponds to a sample, and each column to a class.\n",
        "    \"\"\"\n",
        "    # Create a zero array with shape (number of samples, number of classes)\n",
        "    onehot = np.zeros((len(y), k), dtype=int)\n",
        "    # For each sample, set the column corresponding to its class label to 1\n",
        "    # np.arange(len(y)) creates indices for rows, y for columns\n",
        "    onehot[np.arange(len(y)), y] = 1  # Assuming y are 0-indexed classes\n",
        "    return onehot\n",
        "\n",
        "def L1_reg(lambda_reg, W1, W2):\n",
        "    \"\"\"\n",
        "    Computes the L1 regularization cost. This penalty term is added to the loss function\n",
        "    and is proportional to the sum of the absolute values of the weights (excluding bias terms).\n",
        "    It promotes sparsity in the weight matrices.\n",
        "    Args:\n",
        "        lambda_reg (float): The regularization strength (lambda).\n",
        "        W1 (np.array): Weight matrix for the hidden layer.\n",
        "        W2 (np.array): Weight matrix for the output layer.\n",
        "    Returns:\n",
        "        float: The calculated L1 regularization cost.\n",
        "    \"\"\"\n",
        "    # Sum the absolute values of all weights, excluding the bias column (index 0)\n",
        "    return (lambda_reg / 2) * (np.sum(np.abs(W1[:, 1:])) + np.sum(np.abs(W2[:, 1:])))\n",
        "\n",
        "def L2_reg(lambda_reg, W1, W2):\n",
        "    \"\"\"\n",
        "    Computes the L2 regularization cost. This penalty term is added to the loss function\n",
        "    and is proportional to the sum of the squared values of the weights (excluding bias terms).\n",
        "    It discourages large weights and helps prevent overfitting.\n",
        "    Args:\n",
        "        lambda_reg (float): The regularization strength (lambda).\n",
        "        W1 (np.array): Weight matrix for the hidden layer.\n",
        "        W2 (np.array): Weight matrix for the output layer.\n",
        "    Returns:\n",
        "        float: The calculated L2 regularization cost.\n",
        "    \"\"\"\n",
        "    # Sum the squared values of all weights, excluding the bias column (index 0)\n",
        "    return (lambda_reg / 2) * (np.sum(W1[:, 1:]**2) + np.sum(W2[:, 1:]**2))\n",
        "\n",
        "def MLP_CCESM_forward(X, W1, W2):\n",
        "    \"\"\"\n",
        "    Performs the forward pass through the MLP with sigmoid activation in the hidden layer\n",
        "    and softmax activation in the output layer.\n",
        "\n",
        "    Args:\n",
        "        X (np.array): Input data for the current batch, shape (batch_size, n_features).\n",
        "        W1 (np.array): Weight matrix for the hidden layer, shape (n_hidden, n_features + 1).\n",
        "        W2 (np.array): Weight matrix for the output layer, shape (n_output, n_hidden + 1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: rA2 (np.array): Output activations (probabilities) from the softmax layer,\n",
        "                                 shape (batch_size, n_output).\n",
        "               A1_ext (np.array): Hidden layer activations, including the bias term,\n",
        "                                   shape (batch_size, n_hidden + 1).\n",
        "               A0 (np.array): Input activations, including the bias term,\n",
        "                               shape (batch_size, n_features + 1).\n",
        "               rZ1 (np.array): Pre-activation values of the hidden layer,\n",
        "                               shape (batch_size, n_hidden).\n",
        "               rZ2 (np.array): Pre-activation values of the output layer,\n",
        "                               shape (batch_size, n_output).\n",
        "    \"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "\n",
        "    # A0: Input layer activations augmented with a bias term (column of ones).\n",
        "    # This allows the bias to be treated as a weight connected to a constant input of 1.\n",
        "    A0 = np.hstack((np.ones((batch_size, 1)), X)) # Shape: (batch_size, n_features + 1)\n",
        "    \n",
        "    # rZ1: Pre-activation values for the hidden layer.\n",
        "    # Computed by multiplying input activations (A0) with the transpose of hidden layer weights (W1.T).\n",
        "    rZ1 = A0 @ W1.T # Shape: (batch_size, n_hidden)\n",
        "    # A1: Activated hidden layer values using the sigmoid function.\n",
        "    A1 = MLP_sigmoid(rZ1) # Shape: (batch_size, n_hidden)\n",
        "    # A1_ext: Hidden layer activations augmented with a bias term for the output layer.\n",
        "    A1_ext = np.hstack((np.ones((batch_size, 1)), A1)) # Shape: (batch_size, n_hidden + 1)\n",
        "    \n",
        "    # rZ2: Pre-activation values for the output layer.\n",
        "    # Computed by multiplying extended hidden activations (A1_ext) with the transpose of output layer weights (W2.T).\n",
        "    rZ2 = A1_ext @ W2.T # Shape: (batch_size, n_output)\n",
        "    # rA2: Final output layer activations (predictions) using the softmax function.\n",
        "    # Softmax converts logits into a probability distribution over classes.\n",
        "    rA2 = MLP_softmax(rZ2) # Shape: (batch_size, n_output)\n",
        "    \n",
        "    return rA2, A1_ext, A0, rZ1, rZ2\n",
        "\n",
        "def get_CCE_cost(y_enc, y_pred, model, W1, W2):\n",
        "    \"\"\"\n",
        "    Computes the Categorical Cross-Entropy (CCE) cost, including L1 and L2 regularization terms.\n",
        "    This function measures the performance of a classification model whose output is a probability value\n",
        "    between 0 and 1. It's suitable for multi-class classification when true labels are one-hot encoded.\n",
        "    Args:\n",
        "        y_enc (np.array): One-hot encoded true labels, shape (batch_size, n_output_classes).\n",
        "        y_pred (np.array): Predicted probabilities from the softmax layer, shape (batch_size, n_output_classes).\n",
        "        model (dict): Dictionary containing model parameters 'l1' (L1 regularization weight) and 'l2' (L2 regularization weight).\n",
        "        W1 (np.array): Weight matrix for the hidden layer.\n",
        "        W2 (np.array): Weight matrix for the output layer.\n",
        "    Returns:\n",
        "        float: The total cost, which is the sum of CCE loss and regularization terms.\n",
        "    \"\"\"\n",
        "    # Clip predictions to a small positive value to prevent log(0) which results in NaN or infinite loss.\n",
        "    # Clipping also prevents values from reaching 1.0 which would make log(1-p) problematic in BCE (not CCE here).\n",
        "    y_pred = np.clip(y_pred, 1e-12, 1.0 - 1e-12)\n",
        "\n",
        "    # Calculate the Categorical Cross-Entropy (CCE) loss.\n",
        "    # The formula is -sum(y_true * log(y_pred)) over all classes and all samples in the batch.\n",
        "    # np.sum() without specifying axis will sum all elements of the resulting matrix.\n",
        "    cce_loss = -np.sum(y_enc * np.log(y_pred))\n",
        "\n",
        "    # Add L1 and L2 regularization terms to the cost.\n",
        "    l1 = model['l1']\n",
        "    l2 = model['l2']\n",
        "    L1_term = L1_reg(l1, W1, W2)\n",
        "    L2_term = L2_reg(l2, W1, W2)\n",
        "    \n",
        "    cost = cce_loss + L1_term + L2_term\n",
        "    return cost\n",
        "\n",
        "def get_CCESM_gradient(rA2, A1_ext, A0, rZ1, Y_enc, W1, W2, l1, l2):\n",
        "    \"\"\"\n",
        "    Computes the gradients for the weights of the MLP using the backpropagation algorithm.\n",
        "    This specific implementation is for Categorical Cross-Entropy loss with Softmax output activation.\n",
        "    Args:\n",
        "        rA2 (np.array): Output activations (predicted probabilities) from the softmax layer,\n",
        "                        shape (batch_size, n_output_classes).\n",
        "        A1_ext (np.array): Hidden layer activations, including the bias term,\n",
        "                           shape (batch_size, n_hidden + 1).\n",
        "        A0 (np.array): Input activations, including the bias term,\n",
        "                       shape (batch_size, n_features + 1).\n",
        "        rZ1 (np.array): Pre-activation values of the hidden layer, shape (batch_size, n_hidden).\n",
        "        Y_enc (np.array): One-hot encoded true labels, shape (batch_size, n_output_classes).\n",
        "        W1 (np.array): Current weight matrix for the hidden layer.\n",
        "        W2 (np.array): Current weight matrix for the output layer.\n",
        "        l1 (float): L1 regularization strength.\n",
        "        l2 (float): L2 regularization strength.\n",
        "    Returns:\n",
        "        tuple: delta_W1_unscaled (np.array): Gradient for W1 (excluding regularization).\n",
        "               delta_W2_unscaled (np.array): Gradient for W2 (excluding regularization).\n",
        "    \"\"\"\n",
        "    # Step 1: Compute dL_dZ2 (Error signal at the output layer's pre-activation).\n",
        "    # For CCE loss with Softmax, this derivative simplifies to (predicted_probabilities - true_one_hot_labels).\n",
        "    dL_dZ2 = rA2 - Y_enc # Shape: (batch_size, n_output)\n",
        "    \n",
        "    # Step 2: Compute dL_dW2 (Gradient for the output layer weights W2).\n",
        "    # This is calculated as the dot product of the transpose of the output error (dL_dZ2.T)\n",
        "    # and the activations of the previous layer (A1_ext).\n",
        "    dL_dW2 = dL_dZ2.T @ A1_ext # Shape: (n_output, n_hidden + 1)\n",
        "\n",
        "    # Step 3: Compute dL_dA1 (Error propagated back to the hidden layer's activations).\n",
        "    # This is done by multiplying the output error (dL_dZ2) with the output weights (W2).\n",
        "    dL_dA1 = dL_dZ2 @ W2 # Shape: (batch_size, n_hidden + 1)\n",
        "    \n",
        "    # Step 4: Compute dL_drZ1 (Error signal at the hidden layer's pre-activation).\n",
        "    # This involves taking the portion of dL_dA1 corresponding to actual neurons (excluding bias),\n",
        "    # and element-wise multiplying it by the derivative of the hidden layer's activation function (sigmoid_gradient(rZ1)).\n",
        "    sigma_prime_of_rZ1 = MLP_sigmoid_gradient(rZ1) # Shape: (batch_size, n_hidden)\n",
        "    dL_drZ1 = dL_dA1[:, 1:] * sigma_prime_of_rZ1 # Shape: (batch_size, n_hidden)\n",
        "    \n",
        "    # Step 5: Compute dL_dW1 (Gradient for the hidden layer weights W1).\n",
        "    # This is calculated as the dot product of the transpose of the hidden error (dL_drZ1.T)\n",
        "    # and the input activations (A0).\n",
        "    dL_dW1 = dL_drZ1.T @ A0 # Shape: (n_hidden, n_features + 1)\n",
        "\n",
        "    # Step 6: Add regularization terms to the gradients (for non-bias weights).\n",
        "    # The regularization terms' derivatives are added to the computed gradients.\n",
        "    # Note: Regularization is applied only to the actual weights (columns from index 1 onwards),\n",
        "    # not to the bias terms (column at index 0).\n",
        "    delta_W1_unscaled = dL_dW1.copy()\n",
        "    delta_W2_unscaled = dL_dW2.copy()\n",
        "    \n",
        "    # L1 and L2 derivatives are added to the non-bias parts of the gradients\n",
        "    delta_W1_unscaled[:, 1:] += (l1 * np.sign(W1[:, 1:]) + 2 * l2 * W1[:, 1:])\n",
        "    delta_W2_unscaled[:, 1:] += (l1 * np.sign(W2[:, 1:]) + 2 * l2 * W2[:, 1:])\n",
        "\n",
        "    return delta_W1_unscaled, delta_W2_unscaled\n",
        "\n",
        "def MLP_CCESM_predict(X, W1, W2):\n",
        "    \"\"\"\n",
        "    Predicts class labels for a given input dataset X using the trained MLP.\n",
        "    The prediction is made by performing a forward pass and then selecting the class\n",
        "    with the highest probability from the softmax output.\n",
        "    Args:\n",
        "        X (np.array): Input data for prediction, shape (N_samples, n_features).\n",
        "        W1 (np.array): Trained weight matrix for the hidden layer.\n",
        "        W2 (np.array): Trained weight matrix for the output layer.\n",
        "    Returns:\n",
        "        np.array: Predicted 0-indexed class labels, shape (N_samples,).\n",
        "    \"\"\"\n",
        "    # Perform forward pass to get output probabilities from the softmax layer\n",
        "    rA2, _, _, _, _ = MLP_CCESM_forward(X, W1, W2)\n",
        "    # Select the class with the highest probability for each sample\n",
        "    y_pred = np.argmax(rA2, axis=1) \n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "mlp_core_functions_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. MLP Training Loop**\n",
        "\n",
        "This section defines the core training function for the MLP. The `MLP_CCESM_train` function orchestrates the iterative learning process, employing mini-batch Gradient Descent with momentum. It manages the adaptive learning rate, shuffles the training data for each epoch, processes data in mini-batches, computes loss and gradients, and updates the network's weights. The training progress is monitored by recording the cost history."
      ],
      "metadata": {
        "id": "mlp_training_loop_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_CCESM_train(X_train, y_train, model):\n",
        "    \"\"\"\n",
        "    Trains the Multi-Layer Perceptron model for multi-class classification.\n",
        "    It uses mini-batch Gradient Descent with momentum and an adaptive learning rate.\n",
        "    Args:\n",
        "        X_train (np.array): Training input features, shape (N_train_samples, n_features).\n",
        "        y_train (np.array): True training labels, shape (N_train_samples,).\n",
        "        model (dict): Dictionary containing model parameters:\n",
        "                      'n_output' (int): Number of output classes.\n",
        "                      'l1' (float): L1 regularization weight.\n",
        "                      'l2' (float): L2 regularization weight.\n",
        "                      'eta' (float): Initial learning rate.\n",
        "                      'alpha' (float): Momentum coefficient.\n",
        "                      'epochs' (int): Number of full passes over the training data.\n",
        "                      'minibatches' (int): Number of mini-batches to divide the training data into per epoch.\n",
        "                      'decrease_const' (float): Constant for learning rate decay.\n",
        "    Returns:\n",
        "        tuple: model (dict) - The updated model dictionary containing trained weights (W1, W2)\n",
        "                               and the cost history ('cost_history').\n",
        "               W1 (np.array) - The trained weight matrix for the hidden layer.\n",
        "               W2 (np.array) - The trained weight matrix for the output layer.\n",
        "    \"\"\"\n",
        "    # Initialize weight matrices using the specified model parameters.\n",
        "    W1, W2 = MLP_initialize_weights(model)\n",
        "    \n",
        "    # Extract relevant model parameters for easier access within the training loop.\n",
        "    l1 = model['l1']\n",
        "    l2 = model['l2']\n",
        "    eta = model['eta']\n",
        "    alpha = model['alpha']\n",
        "    epochs = model['epochs']\n",
        "    n_output = model['n_output']\n",
        "    minibatches = model['minibatches']\n",
        "    decrease_const = model['decrease_const']\n",
        "\n",
        "    # One-hot encode all training labels once at the beginning.\n",
        "    # This converts class labels (e.g., 0, 1, ..., 9) into binary vectors (e.g., [1,0,0,...]).\n",
        "    Y_enc = encode_labels(y_train, n_output) # Shape: (N_train_samples, n_output)\n",
        "\n",
        "    # Initialize previous weight changes for the momentum term. Set to zeros initially.\n",
        "    delta_W1_prev = np.zeros_like(W1)\n",
        "    delta_W2_prev = np.zeros_like(W2)\n",
        "    \n",
        "    # Initialize list to store the cost (loss) values observed during training.\n",
        "    # This helps in monitoring convergence and debugging.\n",
        "    model['cost_history'] = []\n",
        "\n",
        "    num_observations = X_train.shape[0]\n",
        "    \n",
        "    # --- Main Training Loop: Iterates over each epoch ---\n",
        "    for e in range(1, epochs + 1):\n",
        "        print(f'Epoch: {e}')\n",
        "        \n",
        "        # Implement adaptive learning rate decay.\n",
        "        # The learning rate decreases with each epoch, allowing for larger steps initially\n",
        "        # and finer adjustments as the model approaches convergence.\n",
        "        current_eta = eta / (1 + decrease_const * e)\n",
        "        \n",
        "        # Shuffle the training data indices at the start of each epoch.\n",
        "        # This ensures that mini-batches are randomly sampled, preventing cyclical patterns\n",
        "        # in weight updates and promoting better generalization.\n",
        "        shuffled_indices = np.random.permutation(num_observations)\n",
        "        X_shuffled = X_train[shuffled_indices, :]\n",
        "        Y_enc_shuffled = Y_enc[shuffled_indices, :]\n",
        "\n",
        "        # Calculate the size of each mini-batch.\n",
        "        batch_size_per_minibatch = num_observations // minibatches\n",
        "        \n",
        "        # --- Mini-batch Loop: Iterates over each mini-batch within the current epoch ---\n",
        "        for m_idx in range(minibatches):\n",
        "            # Determine the start and end indices for the current mini-batch.\n",
        "            start_idx = m_idx * batch_size_per_minibatch\n",
        "            end_idx = start_idx + batch_size_per_minibatch\n",
        "            \n",
        "            # Extract the mini-batch data and corresponding one-hot encoded labels.\n",
        "            X_mini_batch = X_shuffled[start_idx:end_idx, :]\n",
        "            Y_enc_mini_batch = Y_enc_shuffled[start_idx:end_idx, :]\n",
        "            \n",
        "            # Perform the forward pass to get predictions for the current mini-batch.\n",
        "            rA2, A1_ext, A0, rZ1, rZ2 = MLP_CCESM_forward(X_mini_batch, W1, W2)\n",
        "            \n",
        "            # Compute the cost (loss) for the current mini-batch.\n",
        "            cost = get_CCE_cost(Y_enc_mini_batch, rA2, model, W1, W2)\n",
        "            model['cost_history'].append(cost)\n",
        "            \n",
        "            # Compute gradients for weights using backpropagation.\n",
        "            delta_W1_unscaled, delta_W2_unscaled = get_CCESM_gradient(\n",
        "                rA2, A1_ext, A0, rZ1, Y_enc_mini_batch, W1, W2, l1, l2\n",
        "            )\n",
        "            \n",
        "            # Calculate the actual weight changes including the learning rate.\n",
        "            delta_W1 = current_eta * delta_W1_unscaled\n",
        "            delta_W2 = current_eta * delta_W2_unscaled\n",
        "            \n",
        "            # Update the weight matrices using the gradient descent rule with momentum.\n",
        "            # Momentum helps to smooth out updates and accelerate convergence in relevant directions.\n",
        "            W1 = W1 - (delta_W1 + (alpha * delta_W1_prev))\n",
        "            W2 = W2 - (delta_W2 + (alpha * delta_W2_prev))\n",
        "            \n",
        "            # Store the current weight changes to be used as previous changes in the next iteration (for momentum).\n",
        "            delta_W1_prev = delta_W1\n",
        "            delta_W2_prev = delta_W2\n",
        "            \n",
        "    # Store the final trained weight matrices in the model dictionary.\n",
        "    model['W1'] = W1\n",
        "    model['W2'] = W2\n",
        "    return model, W1, W2\n"
      ],
      "metadata": {
        "id": "mlp_training_loop_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Main Script: Training and Evaluation**\n",
        "\n",
        "This section serves as the main execution block for the MLP. It initializes the dataset, sets up model parameters, initiates the training process, and then evaluates the trained model's performance. Key metrics such as training and test accuracy are calculated, and the evolution of the training loss is plotted to observe convergence. A confusion matrix is also generated to provide a detailed breakdown of classification performance per class."
      ],
      "metadata": {
        "id": "main_script_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close('all') # Close all existing matplotlib figures to ensure clean plots\n",
        "\n",
        "# --- Parameters for MNIST Data Loading ---\n",
        "classes = list(range(10)) # All 10 digits (0-9) for classification\n",
        "Ntrain = 60000            # Total number of training patterns to load\n",
        "Ntest  = 10000            # Total number of test patterns to load (handled by mnist_load)\n",
        "\n",
        "# --- Load MNIST Data ---\n",
        "print(\"Loading MNIST dataset...\")\n",
        "Xtrain, ytrain, Xtest, ytest = mnist_load(classes, Ntrain)\n",
        "print(f\"Training data loaded: Xtrain shape={Xtrain.shape}, ytrain shape={ytrain.shape}\")\n",
        "print(f\"Test data loaded: Xtest shape={Xtest.shape}, ytest shape={ytest.shape}\")\n",
        "\n",
        "# --- Display a Sample of Training Images ---\n",
        "print(\"\\nDisplaying a sample of MNIST training digits:\")\n",
        "# Select 64 random images to display in an 8x8 grid\n",
        "idx = np.random.permutation(Xtrain.shape[0])[:64]\n",
        "display_mnist(Xtrain[idx, :])\n",
        "\n",
        "# --- Model Parameters for MLP ---\n",
        "model = {\n",
        "    'n_output': len(classes),      # Number of output neurons (equal to number of classes, 10 for MNIST)\n",
        "    'n_features': Xtrain.shape[1], # Number of input features (28*28 = 784 pixels per image)\n",
        "    'n_hidden': 50,                # Number of neurons in the single hidden layer (can be tuned)\n",
        "    'l1': 0,                       # L1 regularization weight (set to 0 for no L1 regularization)\n",
        "    'l2': 0.1,                     # L2 regularization weight (a small value to prevent overfitting)\n",
        "    'epochs': 50,                  # Number of full passes through the training dataset\n",
        "    'eta': 0.001,                  # Initial learning rate (step size for weight updates)\n",
        "    'alpha': 0.001,                # Momentum coefficient (helps accelerate GD in relevant directions)\n",
        "    'decrease_const': 0.00001,     # Constant for adaptive learning rate decay (eta / (1 + decrease_const * epoch))\n",
        "    'minibatches': 50,             # Number of mini-batches per epoch. `Xtrain.shape[0] // 50` means approx 1200 samples per batch.\n",
        "}\n",
        "\n",
        "# --- Train MLP ---\n",
        "print(\"\\nStarting MLP training on MNIST dataset...\")\n",
        "model_trained, W1_trained, W2_trained = MLP_CCESM_train(Xtrain, ytrain, model)\n",
        "print(\"MLP training complete.\")\n",
        "\n",
        "# --- Make Predictions on Training and Test Sets ---\n",
        "ytrain_pred = MLP_CCESM_predict(Xtrain, W1_trained, W2_trained)\n",
        "ytest_pred  = MLP_CCESM_predict(Xtest,  W1_trained, W2_trained)\n",
        "\n",
        "# --- Compute and Display Accuracy ---\n",
        "acc_train = np.mean(ytrain == ytrain_pred) # Calculate average accuracy over training set\n",
        "print(f'\\nTraining accuracy: {acc_train * 100:.2f}%')\n",
        "\n",
        "acc_test = np.mean(ytest == ytest_pred)   # Calculate average accuracy over test set\n",
        "print(f'Test accuracy: {acc_test * 100:.2f}%')\n",
        "\n",
        "# --- Plot Training Loss History ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(model_trained['cost_history'], color='blue', linewidth=2)\n",
        "plt.title('MLP Training Loss over Mini-batch Iterations')\n",
        "plt.xlabel('Mini-batch Iteration')\n",
        "plt.ylabel('Loss (Categorical Cross-Entropy)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Compute and Display Confusion Matrix for Test Set ---\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "num_classes = model['n_output']\n",
        "confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "# Populate the confusion matrix\n",
        "for i in range(len(ytest)):\n",
        "    true_label = ytest[i]\n",
        "    predicted_label = ytest_pred[i]\n",
        "    confusion_matrix[true_label, predicted_label] += 1\n",
        "\n",
        "print(confusion_matrix)\n",
        "\n",
        "# --- Visualize Confusion Matrix (Confusion Chart) ---\n",
        "plt.figure(figsize=(8, 7))\n",
        "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues) # Use a blue colormap\n",
        "plt.title('Confusion Chart (Test Set)')\n",
        "plt.colorbar() # Add a color bar to indicate values\n",
        "\n",
        "# Set ticks and labels for axes\n",
        "tick_marks = np.arange(num_classes)\n",
        "plt.xticks(tick_marks, np.arange(num_classes))\n",
        "plt.yticks(tick_marks, np.arange(num_classes))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "# Add text annotations to each cell of the confusion matrix\n",
        "thresh = confusion_matrix.max() / 2. # Threshold for text color (white on dark, black on light)\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        plt.text(j, i, format(confusion_matrix[i, j], 'd'),\n",
        "                 ha=\"center\", va=\"center\",\n",
        "                 color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "main_script_code"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Visualization of Model Performance on Test Samples**\n",
        "\n",
        "For high-dimensional input data like MNIST digits (784 features), a 3D decision surface plot is not directly interpretable. Instead, this section visualizes the model's performance by displaying a selection of test images along with their true labels and the MLP's predicted labels. Misclassified digits are highlighted to provide insights into where the model struggles. This direct visual inspection of predictions on actual data samples offers a more practical understanding of the MLP's learned capabilities."
      ],
      "metadata": {
        "id": "sample_predictions_visualization_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVisualizing sample test predictions:\")\n",
        "\n",
        "# Select a random subset of test images to display\n",
        "num_display_samples = 25 # Display 25 samples in a 5x5 grid\n",
        "display_indices = np.random.choice(len(ytest), num_display_samples, replace=False)\n",
        "\n",
        "fig_pred, axes_pred = plt.subplots(5, 5, figsize=(10, 10))\n",
        "axes_pred = axes_pred.flatten()\n",
        "\n",
        "for i, ax in enumerate(axes_pred):\n",
        "    if i < num_display_samples:\n",
        "        img_idx = display_indices[i]\n",
        "        image = Xtest[img_idx, :].reshape(28, 28) # Reshape 1D vector to 2D image\n",
        "        true_label = ytest[img_idx]\n",
        "        predicted_label = ytest_pred[img_idx]\n",
        "\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "        title_color = 'green' if true_label == predicted_label else 'red'\n",
        "        ax.set_title(f'True: {true_label}\\nPred: {predicted_label}', color=title_color, fontsize=10)\n",
        "    else:\n",
        "        ax.axis('off') # Hide unused subplots if num_display_samples is less than 25\n",
        "\n",
        "plt.suptitle('Test Sample Predictions (Green: Correct, Red: Incorrect)', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sample_predictions_visualization_code"
      }
    }
  ]
}
