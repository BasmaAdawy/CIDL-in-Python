{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Radial Basis Function (RBF) Network for 2D Regression**\n",
        "\n",
        "This notebook demonstrates the implementation of a Radial Basis Function (RBF) network for regression on a 2D synthetic dataset. It covers the process from generating the training and test data, defining the core RBF network architecture and its mathematical underpinnings, training the model using a closed-form solution (least squares), and finally, evaluating its performance with visualizations and metrics."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "setup_code",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 2D Synthetic Data Generation**\n",
        "\n",
        "In this section, we generate a synthetic 2D regression dataset. The target variable is created as a **Mixture of Gaussians (MoG)** in a 2D input space, providing a non-linear surface that the RBF network will learn to approximate. This function generates both the input features (X) and the corresponding target values (y), and can optionally visualize the 3D surface of the generated data."
      ],
      "metadata": {
        "id": "synthetic_data_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mog2d_gen_code"
      },
      "outputs": [],
      "source": [
        "def mog2D_gen(Npatterns=10000, visualize=True):\n",
        "    \"\"\"\n",
        "    Generates a 2D Mixture of Gaussians synthetic dataset\n",
        "\n",
        "    Parameters:\n",
        "    - Npatterns: Number of data points (should be a perfect square)\n",
        "    - visualize: Whether to plot the generated data\n",
        "\n",
        "    Returns:\n",
        "    - X: Input features (Npatterns x 2)\n",
        "    - y: Target values (Npatterns x 1)\n",
        "    - XX1, XX2: Meshgrid coordinates (for visualization)\n",
        "    - YY: Meshgrid target values (for visualization)\n",
        "    \"\"\"\n",
        "    n = int(np.sqrt(Npatterns))\n",
        "    x1 = np.linspace(-10, 10, n)\n",
        "    x2 = np.linspace(-10, 10, n)\n",
        "    XX1, XX2 = np.meshgrid(x1, x2)\n",
        "\n",
        "    # Define parameters for three 2D Gaussian kernels\n",
        "    centre1, centre2, centre3 = -8, -1, 7\n",
        "    ampl1, ampl2, ampl3 = 3, -5, 9\n",
        "    epsilon = 0.3\n",
        "\n",
        "    # Compute each 2D kernel component\n",
        "    k1 = ampl1 * np.exp(-((XX1 - centre1) * epsilon)**2) * np.exp(-((XX2 - centre1) * epsilon)**2)\n",
        "    k2 = ampl2 * np.exp(-((XX1 - centre2) * epsilon)**2) * np.exp(-((XX2 - centre2) * epsilon)**2)\n",
        "    k3 = ampl3 * np.exp(-((XX1 - centre3) * epsilon)**2) * np.exp(-((XX2 - centre3) * epsilon)**2)\n",
        "\n",
        "    # Combine kernels to form target output\n",
        "    YY = k1 + k2 + k3\n",
        "\n",
        "    # Flatten for training data\n",
        "    X = np.column_stack([XX1.ravel(), XX2.ravel()])\n",
        "    y = YY.ravel()\n",
        "\n",
        "    if visualize:\n",
        "        fig = plt.figure(figsize=(10, 7))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        surf = ax.plot_surface(XX1, XX2, YY, cmap='viridis')\n",
        "        fig.colorbar(surf)\n",
        "        ax.set_title('2D Mixture of Gaussians Synthetic Dataset', fontsize=14)\n",
        "        ax.set_xlabel('X1', fontsize=12)\n",
        "        ax.set_ylabel('X2', fontsize=12)\n",
        "        ax.set_zlabel('y', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "    return X, y, XX1, XX2, YY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. RBF Network Core Functions**\n",
        "\n",
        "This section defines the fundamental building blocks of our RBF network. An RBF network consists of an input layer, a hidden layer of RBF units, and a linear output layer. The training process typically involves initializing the RBF centers (e.g., via clustering) and then finding the optimal output layer weights through a least squares method."
      ],
      "metadata": {
        "id": "rbf_core_functions_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Key Formulas**\n",
        "\n",
        "The RBF network relies on two primary mathematical components:\n",
        "\n",
        "### **Gaussian Radial Basis Function (RBF)**\n",
        "This function is used in the hidden layer to measure the similarity between an input vector $\\mathbf{x}$ and a center vector $\\mathbf{c}$. The output is highest when $\\mathbf{x}$ is close to $\\mathbf{c}$ and decreases as the distance increases, controlled by the width parameter $\\epsilon$:\n",
        "$$\\phi(\\mathbf{x}, \\mathbf{c}, \\epsilon) = e^{-\\epsilon^2 \\|\\mathbf{x} - \\mathbf{c}\\|^2}$$\n",
        "where:\n",
        "* $\\mathbf{x}$ is the input vector (e.g., $[x_1, x_2]$ for a 2D input).\n",
        "* $\\mathbf{c}$ is the center vector of the radial basis function for a specific hidden unit.\n",
        "* $\\epsilon$ is the width parameter, controlling the spread of the Gaussian (a larger $\\epsilon$ means a narrower function).\n",
        "* $\\|\\cdot\\|$ denotes the Euclidean norm (distance).\n",
        "\n",
        "### **Network Output (Linear Output Layer)**\n",
        "The hidden layer's outputs, which are the activations of each RBF unit for a given input, are then linearly combined to produce the final network output. A bias term is typically included. For a set of $N$ input patterns, the predictions $\\hat{\\mathbf{Y}}$ are calculated as:\n",
        "$$\\mathbf{\\hat{Y}} = \\mathbf{H}_{ext} \\mathbf{W}_2$$\n",
        "where:\n",
        "* $\\mathbf{\\hat{Y}}$ is the column vector of predicted outputs (N x 1).\n",
        "* $\\mathbf{H}_{ext}$ is the matrix of extended hidden layer outputs (N x (Number of Hidden Units + 1)). Each row corresponds to an input sample, with the first column being a bias of 1, and subsequent columns being the activations of the RBF units for that input.\n",
        "* $\\mathbf{W}_2$ is the column vector of output weights ((Number of Hidden Units + 1) x 1), including the bias weight."
      ],
      "metadata": {
        "id": "rbf_formulas_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RBF_train_offline(X, y, model_params):\n",
        "    \"\"\"\n",
        "    Trains an RBF network offline (batch mode).\n",
        "    This function performs two main steps:\n",
        "    1. Initializes the RBF centers using either random selection or k-means clustering.\n",
        "    2. Computes the output layer weights (W2) by solving a least squares problem.\n",
        "    \"\"\"\n",
        "    n_hidden = model_params['n_hidden']\n",
        "    n_features = model_params['n_features']\n",
        "\n",
        "    # Step 1: Initialize RBF centers (W1)\n",
        "    if model_params['centres_generation_method'] == 'random':\n",
        "        # Randomly select training points as centers\n",
        "        idx = np.random.choice(X.shape[0], n_hidden, replace=False)\n",
        "        W1 = X[idx, :]\n",
        "    elif model_params['centres_generation_method'] == 'clustering':\n",
        "        # Use k-means clustering to find centers\n",
        "        from sklearn.cluster import KMeans\n",
        "        print('Clustering training data using K-means to initialize centers...')\n",
        "        kmeans = KMeans(n_clusters=n_hidden, random_state=42, n_init=10).fit(X) # Added n_init for robustness\n",
        "        W1 = kmeans.cluster_centers_\n",
        "        print('K-means clustering completed.')\n",
        "\n",
        "    # Step 2: Compute RBF activations (H)\n",
        "    epsilon = model_params['epsilon']\n",
        "    # Calculate Euclidean distance between each input sample and each center\n",
        "    distances = np.sqrt(((X[:, np.newaxis, :] - W1[np.newaxis, :, :])**2).sum(axis=2))\n",
        "    # Apply Gaussian RBF activation function\n",
        "    H = np.exp(-(distances * epsilon)**2)\n",
        "\n",
        "    # Add bias term to the hidden layer output matrix (H_ext)\n",
        "    H = np.column_stack([np.ones(X.shape[0]), H])\n",
        "\n",
        "    # Step 3: Solve for output weights (W2) using least squares\n",
        "    # W2 = (H_ext^T * H_ext)^-1 * H_ext^T * y\n",
        "    # Using np.linalg.pinv (Moore-Penrose pseudo-inverse) for robustness\n",
        "    W2 = np.linalg.pinv(H) @ y.reshape(-1, 1)\n",
        "\n",
        "    return model_params, W1, W2\n"
      ],
      "metadata": {
        "id": "T5a-M9rntymG",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RBF_predict(X, W1, W2, epsilon):\n",
        "    \"\"\"\n",
        "    Makes predictions using a trained RBF network.\n",
        "    This function performs the forward pass through the RBF network to compute outputs.\n",
        "    \"\"\"\n",
        "    # Compute RBF activations for the given input X\n",
        "    # Calculate Euclidean distance between each input sample and each center\n",
        "    distances = np.sqrt(((X[:, np.newaxis, :] - W1[np.newaxis, :, :])**2).sum(axis=2))\n",
        "    # Apply Gaussian RBF activation function\n",
        "    H = np.exp(-(distances * epsilon)**2)\n",
        "\n",
        "    # Add bias term to the hidden layer output matrix\n",
        "    H = np.column_stack([np.ones(X.shape[0]), H])\n",
        "\n",
        "    # Compute predictions by multiplying hidden layer outputs with output weights\n",
        "    y_pred = H @ W2\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "dPuWOaxOt03A",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Main Execution: 2D RBF Network Training and Evaluation**\n",
        "\n",
        "This is the main script that orchestrates the entire RBF network workflow. It begins by generating the 2D synthetic data, sets up the RBF model's configuration parameters, trains the network, and then evaluates its performance using visualizations and quantitative metrics like Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "main_script_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Generation and Visualization"
      ],
      "metadata": {
        "id": "XvpwGwrFs9xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training and test data\n",
        "Ntrain = 150*150  # 22500 points (150x150 grid)\n",
        "Ntest = 2500      # 2500 points (50x50 grid)\n",
        "\n",
        "Xtrain, ytrain, XX1_train, XX2_train, YY_train = mog2D_gen(Ntrain, True)\n",
        "Xtest, ytest, XX1_test, XX2_test, YY_test = mog2D_gen(Ntest, False)\n",
        "\n",
        "# Shuffle training data to ensure randomness for training process (if batching were used)\n",
        "shuffled_ind = np.random.permutation(Xtrain.shape[0])\n",
        "Xtrain = Xtrain[shuffled_ind, :]\n",
        "ytrain = ytrain[shuffled_ind]"
      ],
      "metadata": {
        "id": "77ZMd402tAx-",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Configuration"
      ],
      "metadata": {
        "id": "ea4ovVF-tEdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model parameters\n",
        "model = {\n",
        "    'n_output': 1,              # Single output regression\n",
        "    'n_features': 2,            # 2D input (X1, X2)\n",
        "    'n_hidden': 50,             # Number of RBF centers (hidden units)\n",
        "    'epsilon': 0.1,             # RBF width parameter: controls the 'tightness' of the RBFs\n",
        "    'centres_generation_method': 'clustering'  # Method to initialize RBF centers: 'random' or 'clustering'\n",
        "}"
      ],
      "metadata": {
        "id": "aTjzZ-pStIyN",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "0ue2eEmctMTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RBF network using the offline (batch) method\n",
        "print(\"\\nStarting RBF network training...\")\n",
        "model, W1, W2 = RBF_train_offline(Xtrain, ytrain, model)\n",
        "print(\"RBF network training complete.\")\n"
      ],
      "metadata": {
        "id": "BONcGABptPO4",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "8vxPIs_RtSb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on both the training and test datasets\n",
        "ytrain_pred = RBF_predict(Xtrain, W1, W2, model['epsilon'])\n",
        "ytest_pred = RBF_predict(Xtest, W1, W2, model['epsilon'])\n",
        "\n",
        "# Reshape predictions to match ytrain/ytest shapes for plotting\n",
        "ytrain_pred = ytrain_pred.ravel()  # Flatten to (N_samples,)\n",
        "ytest_pred = ytest_pred.ravel()    # Flatten to (N_samples,)\n",
        "\n",
        "# Visualize results: 3D scatter plots of true vs predicted values\n",
        "fig = plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Training set predictions visualization\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax1.scatter(Xtrain[:, 0], Xtrain[:, 1], ytrain, c='b', label='True values', alpha=0.6)\n",
        "ax1.scatter(Xtrain[:, 0], Xtrain[:, 1], ytrain_pred, c='r', alpha=0.3, label='Predictions')\n",
        "ax1.set_title('Training Set: True vs Predicted', fontsize=12)\n",
        "ax1.set_xlabel('X1', fontsize=10)\n",
        "ax1.set_ylabel('X2', fontsize=10)\n",
        "ax1.set_zlabel('y', fontsize=10)\n",
        "ax1.legend()\n",
        "\n",
        "# Test set predictions visualization\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "ax2.scatter(Xtest[:, 0], Xtest[:, 1], ytest, c='b', label='True values', alpha=0.6)\n",
        "ax2.scatter(Xtest[:, 0], Xtest[:, 1], ytest_pred, c='r', alpha=0.3, label='Predictions')\n",
        "ax2.set_title('Test Set: True vs Predicted', fontsize=12)\n",
        "ax2.set_xlabel('X1', fontsize=10)\n",
        "ax2.set_ylabel('X2', fontsize=10)\n",
        "ax2.set_zlabel('y', fontsize=10)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Scatter plots of true vs predicted values (2D view)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ytrain, ytrain_pred, '.b', alpha=0.5) # x-axis: True y, y-axis: Predicted y\n",
        "plt.title('Training Set Predictions (True vs Predicted)', fontsize=12)\n",
        "plt.xlabel('True y', fontsize=10)\n",
        "plt.ylabel('Predicted y', fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.axis('equal') # Ensures a square plot for better comparison\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ytest, ytest_pred, '.r', alpha=0.5) # x-axis: True y, y-axis: Predicted y\n",
        "plt.title('Test Set Predictions (True vs Predicted)', fontsize=12)\n",
        "plt.xlabel('True y', fontsize=10)\n",
        "plt.ylabel('Predicted y', fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fhy7iouJtYSt",
        "execution_count": null,
        "outputs": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Metrics"
      ],
      "metadata": {
        "id": "xyoZDr3AtdmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Mean Squared Error (MSE) for both training and test sets\n",
        "mse_train = np.mean((ytrain - ytrain_pred)**2)\n",
        "mse_test = np.mean((ytest - ytest_pred)**2)\n",
        "\n",
        "print(f'Training MSE: {mse_train:.4f}')\n",
        "print(f'Test MSE: {mse_test:.4f}')"
      ],
      "metadata": {
        "id": "WmOTD2yFthI6",
        "execution_count": null,
        "outputs": []
      }
    }
  ]
}
